{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c2191fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/home/ubuntu/spark-3.2.1-bin-hadoop2.7')\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88786ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/ubuntu/spark-3.2.1-bin-hadoop2.7/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/10/11 23:16:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"diabetes_indicators\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ceadd76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/11 23:17:13 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-----------+------+-----+-------+--------+-------------+-------------+--------+--------+--------+--------+--------+------+--------+--------------------+--------+--------+--------+------+--------+--------+------+--------+-------+-------+--------+--------+--------+--------+--------+-------+--------+-------+------+--------+-------+-------+--------+--------+--------+-------+-------+--------+--------+--------+--------+--------+--------+--------+--------+---+-------+-----+--------+--------+--------+-------+--------+-------+--------+-------+--------+-------+-------+--------+--------+--------+-----+------+--------+--------+--------+--------+--------+--------+--------+-------+-------+--------+--------+--------+--------+------+-------+-------+-------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+-------+--------+--------+--------+--------+-------+--------+--------+--------+--------+-------+-------+-------+-------+--------+--------+--------+-------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+-------+--------+------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+-------+--------+-------+-------+--------+--------+--------+------+-------+-------+--------+-------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+-------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+------+-------+--------+------+------+-------+------+-------+-------+------+-------+-------------------+------------+------+-------+------------------+--------+------------------+--------+-------+-------+--------+--------+------------------+------------------+-------+--------+--------+--------+-------+------+--------+--------+--------+--------+-------+-------+--------+-----+--------+--------+--------+--------+--------+------+------+-----+-----+-------+------+--------+-------+--------+-------+-------+--------+--------+--------+--------------------+--------+--------------------+--------+--------------------+--------+--------------------+--------+--------+--------+--------------------+--------------------+--------------------+--------------------+--------+--------+-------+-------+------+------+--------------------+--------------------+--------+--------+--------------------+------------------+------------------+--------+--------------------+-------+-------+--------+--------+--------+--------------------+--------------------+--------------------+--------+--------------------+-------+--------------------+--------------------+--------------------+-------+--------+--------+--------+--------+--------+-------+--------+--------+--------+--------+--------+--------+--------+--------+--------+\n",
      "|_STATE|FMONTH|      IDATE|IMONTH| IDAY|  IYEAR|DISPCODE|        SEQNO|         _PSU|CTELENUM|PVTRESD1|COLGHOUS|STATERES|CELLFON3|LADULT|NUMADULT|              NUMMEN|NUMWOMEN|CTELNUM1|CELLFON2|CADULT|PVTRESD2|CCLGHOUS|CSTATE|LANDLINE|HHADULT|GENHLTH|PHYSHLTH|MENTHLTH|POORHLTH|HLTHPLN1|PERSDOC2|MEDCOST|CHECKUP1|BPHIGH4|BPMEDS|BLOODCHO|CHOLCHK|TOLDHI2|CVDINFR4|CVDCRHD4|CVDSTRK3|ASTHMA3|ASTHNOW|CHCSCNCR|CHCOCNCR|CHCCOPD1|HAVARTH3|ADDEPEV2|CHCKIDNY|DIABETE3|DIABAGE2|SEX|MARITAL|EDUCA|RENTHOM1|NUMHHOL2|NUMPHON2|CPDEMO1|VETERAN3|EMPLOY1|CHILDREN|INCOME2|INTERNET|WEIGHT2|HEIGHT3|PREGNANT|QLACTLM2|USEEQUIP|BLIND|DECIDE|DIFFWALK|DIFFDRES|DIFFALON|SMOKE100|SMOKDAY2|STOPSMK2|LASTSMK2|USENOW3|ALCDAY5|AVEDRNK2|DRNK3GE5|MAXDRNKS|FRUITJU1|FRUIT1|FVBEANS|FVGREEN|FVORANG|VEGETAB1|EXERANY2|EXRACT11|EXEROFT1|EXERHMM1|EXRACT21|EXEROFT2|EXERHMM2|STRENGTH|LMTJOIN3|ARTHDIS2|ARTHSOCL|JOINPAIN|SEATBELT|FLUSHOT6|FLSHTMY2|IMFVPLAC|PNEUVAC3|HIVTST6|HIVTSTD3|WHRTST10|PDIABTST|PREDIAB1|INSULIN|BLDSUGAR|FEETCHK2|DOCTDIAB|CHKHEMO3|FEETCHK|EYEEXAM|DIABEYE|DIABEDU|PAINACT2|QLMENTL2|QLSTRES2|QLHLTH2|CAREGIV1|CRGVREL1|CRGVLNG1|CRGVHRS1|CRGVPRB1|CRGVPERS|CRGVHOUS|CRGVMST2|CRGVEXPT|VIDFCLT2|VIREDIF3|VIPRFVS2|VINOCRE2|VIEYEXM2|VIINSUR2|VICTRCT4|VIGLUMA2|VIMACDG2|CIMEMLOS|CDHOUSE|CDASSIST|CDHELP|CDSOCIAL|CDDISCUS|WTCHSALT|LONGWTCH|DRADVISE|ASTHMAGE|ASATTACK|ASERVIST|ASDRVIST|ASRCHKUP|ASACTLIM|ASYMPTOM|ASNOSLEP|ASTHMED3|ASINHALR|HAREHAB1|STREHAB1|CVDASPRN|ASPUNSAF|RLIVPAIN|RDUCHART|RDUCSTRK|ARTTODAY|ARTHWGT|ARTHEXER|ARTHEDU|TETANUS|HPVADVC2|HPVADSHT|SHINGLE2|HADMAM|HOWLONG|HADPAP2|LASTPAP2|HPVTEST|HPLSTTST|HADHYST2|PROFEXAM|LENGEXAM|BLDSTOOL|LSTBLDS3|HADSIGM3|HADSGCO1|LASTSIG3|PCPSAAD2|PCPSADI1|PCPSARE1|PSATEST1|PSATIME|PCPSARS1|PCPSADE1|PCDMDECN|SCNTMNY1|SCNTMEL1|SCNTPAID|SCNTWRK1|SCNTLPAD|SCNTLWK1|SXORIENT|TRNSGNDR|RCSGENDR|RCSRLTN2|CASTHDX2|CASTHNO2|EMTSUPRT|LSATISFY|ADPLEASR|ADDOWN|ADSLEEP|ADENERGY|ADEAT1|ADFAIL|ADTHINK|ADMOVE|MISTMNT|ADANXEV|QSTVER|QSTLANG|           EXACTOT1|    EXACTOT2|MSCODE| _STSTR|            _STRWT|_RAWRAKE|          _WT2RAKE|_CHISPNC|_CRACE1|_CPRACE|_CLLCPWT|_DUALUSE|          _DUALCOR|           _LLCPWT|_RFHLTH|_HCVU651|_RFHYPE5|_CHOLCHK|_RFCHOL|_MICHD|_LTASTH1|_CASTHM1|_ASTHMS1|_DRDXAR1|_PRACE1|_MRACE1|_HISPANC|_RACE|_RACEG21|_RACEGR3|_RACE_G1|_AGEG5YR|_AGE65YR|_AGE80|_AGE_G|HTIN4| HTM4|  WTKG3| _BMI5|_BMI5CAT|_RFBMI5|_CHLDCNT|_EDUCAG|_INCOMG|_SMOKER3|_RFSMOK3|DRNKANY5|            DROCDY3_|_RFBING5|            _DRNKWEK|_RFDRHV5|            FTJUDA1_|FRUTDA1_|            BEANDAY_|GRENDAY_|ORNGDAY_|VEGEDA1_|            _MISFRTN|            _MISVEGN|            _FRTRESP|            _VEGRESP|_FRUTSUM|_VEGESUM|_FRTLT1|_VEGLT1|_FRT16|_VEG23|            _FRUITEX|            _VEGETEX|_TOTINDA|METVL11_|            METVL21_|           MAXVO2_|             FC60_|ACTIN11_|            ACTIN21_|PADUR1_|PADUR2_|PAFREQ1_|PAFREQ2_|_MINAC11|            _MINAC21|            STRFREQ_|            PAMISS1_|PAMIN11_|            PAMIN21_|PA1MIN_|            PAVIG11_|            PAVIG21_|            PA1VIGM_|_PACAT1|_PAINDX1|_PA150R2|_PA300R2|_PA30021|_PASTRNG|_PAREC1|_PASTAE1|_LMTACT1|_LMTWRK1|_LMTSCL1|_RFSEAT2|_RFSEAT3|_FLSHOT6|_PNEUMO2|_AIDTST3|\n",
      "+------+------+-----------+------+-----+-------+--------+-------------+-------------+--------+--------+--------+--------+--------+------+--------+--------------------+--------+--------+--------+------+--------+--------+------+--------+-------+-------+--------+--------+--------+--------+--------+-------+--------+-------+------+--------+-------+-------+--------+--------+--------+-------+-------+--------+--------+--------+--------+--------+--------+--------+--------+---+-------+-----+--------+--------+--------+-------+--------+-------+--------+-------+--------+-------+-------+--------+--------+--------+-----+------+--------+--------+--------+--------+--------+--------+--------+-------+-------+--------+--------+--------+--------+------+-------+-------+-------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+-------+--------+--------+--------+--------+-------+--------+--------+--------+--------+-------+-------+-------+-------+--------+--------+--------+-------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+-------+--------+------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+-------+--------+-------+-------+--------+--------+--------+------+-------+-------+--------+-------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+-------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+------+-------+--------+------+------+-------+------+-------+-------+------+-------+-------------------+------------+------+-------+------------------+--------+------------------+--------+-------+-------+--------+--------+------------------+------------------+-------+--------+--------+--------+-------+------+--------+--------+--------+--------+-------+-------+--------+-----+--------+--------+--------+--------+--------+------+------+-----+-----+-------+------+--------+-------+--------+-------+-------+--------+--------+--------+--------------------+--------+--------------------+--------+--------------------+--------+--------------------+--------+--------+--------+--------------------+--------------------+--------------------+--------------------+--------+--------+-------+-------+------+------+--------------------+--------------------+--------+--------+--------------------+------------------+------------------+--------+--------------------+-------+-------+--------+--------+--------+--------------------+--------------------+--------------------+--------+--------------------+-------+--------------------+--------------------+--------------------+-------+--------+--------+--------+--------+--------+-------+--------+--------+--------+--------+--------+--------+--------+--------+--------+\n",
      "|   1.0|   1.0|b'01292015'| b'01'|b'29'|b'2015'|  1200.0|2.015000001E9|2.015000001E9|     1.0|     1.0|    null|     1.0|     2.0|  null|     3.0|                 1.0|     2.0|    null|    null|  null|    null|    null|  null|    null|   null|    5.0|    15.0|    18.0|    10.0|     1.0|     1.0|    2.0|     1.0|    1.0|   1.0|     1.0|    1.0|    1.0|     2.0|     2.0|     2.0|    1.0|    1.0|     2.0|     2.0|     1.0|     1.0|     1.0|     2.0|     3.0|    null|2.0|    1.0|  4.0|     1.0|     2.0|    null|    1.0|     2.0|    8.0|    88.0|    3.0|     2.0|  280.0|  510.0|    null|     1.0|     1.0|  2.0|   2.0|     1.0|     1.0|     1.0|     1.0|     3.0|    null|     2.0|    3.0|  888.0|    null|    null|    null|   305.0| 310.0|  320.0|  310.0|  305.0|   101.0|     2.0|    null|    null|    null|    null|    null|    null|   888.0|     1.0|     1.0|     1.0|     6.0|     1.0|     1.0|112014.0|     1.0|     1.0|    1.0|    null|    null|     1.0|     3.0|   null|    null|    null|    null|    null|   null|   null|   null|   null|    null|    null|    null|   null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|   null|    null|  null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|   null|    null|   null|   null|    null|    null|    null|  null|   null|   null|    null|   null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|   null|    null|    null|     b''|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|  null|   null|    null|  null|  null|   null|  null|   null|   null|  10.0|    1.0|                b''|         b''|   3.0|11011.0|28.781560195347467|     3.0|  86.3446805860424|    null|   null|   null|    null|     1.0|0.6141246821308681|341.38485273395895|    2.0|     1.0|     2.0|     1.0|    2.0|   2.0|     2.0|     2.0|     1.0|     1.0|    1.0|    1.0|     2.0|  1.0|     1.0|     1.0|     1.0|     9.0|     1.0|  63.0|   5.0| 70.0|178.0|12701.0|4018.0|     4.0|    2.0|     1.0|    2.0|    2.0|     3.0|     1.0|     2.0|5.397605346934028...|     1.0|5.397605346934028...|     1.0|                17.0|    33.0|                67.0|    33.0|    17.0|   100.0|5.397605346934028...|5.397605346934028...|                 1.0|                 1.0|    50.0|   217.0|    2.0|    1.0|   1.0|   1.0|5.397605346934028...|5.397605346934028...|     2.0|    null|                null|            2469.0|423.00000000000006|    null|                null|   null|   null|    null|    null|    null|                null|5.397605346934028...|5.397605346934028...|    null|                null|   null|                null|                null|                null|    4.0|     2.0|     3.0|     3.0|     2.0|     2.0|    4.0|     2.0|     1.0|     1.0|     1.0|     1.0|     1.0|    null|    null|     1.0|\n",
      "|   1.0|   1.0|b'01202015'| b'01'|b'20'|b'2015'|  1100.0|2.015000002E9|2.015000002E9|     1.0|     1.0|    null|     1.0|     2.0|  null|     1.0|5.397605346934028...|     1.0|    null|    null|  null|    null|    null|  null|    null|   null|    3.0|    88.0|    88.0|    null|     2.0|     1.0|    1.0|     4.0|    3.0|  null|     1.0|    4.0|    2.0|     2.0|     2.0|     2.0|    2.0|   null|     2.0|     2.0|     2.0|     2.0|     2.0|     2.0|     3.0|    null|2.0|    2.0|  6.0|     1.0|     2.0|    null|    2.0|     2.0|    3.0|    88.0|    1.0|     1.0|  165.0|  508.0|    null|     1.0|     2.0|  1.0|   1.0|     2.0|     2.0|     2.0|     1.0|     1.0|     2.0|    null|    3.0|  888.0|    null|    null|    null|   302.0| 305.0|  302.0|  202.0|  202.0|   304.0|     1.0|    64.0|   212.0|   100.0|    69.0|   212.0|   100.0|   888.0|    null|    null|    null|    null|     3.0|     2.0|    null|    null|     2.0|    2.0|    null|    null|     2.0|     3.0|   null|    null|    null|    null|    null|   null|   null|   null|   null|    null|    null|    null|   null|     2.0|    null|    null|    null|    null|    null|    null|    null|     1.0|    null|    null|    null|    null|    null|    null|    null|    null|    null|     1.0|    5.0|     5.0|  null|     5.0|     2.0|     2.0|    null|     2.0|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|   null|    null|   null|   null|    null|    null|    null|  null|   null|   null|    null|   null|    null|    null|    null|    null|     2.0|    null|     2.0|    null|    null|    null|    null|    null|    null|   null|    null|    null|     b''|     1.0|     2.0|    null|    null|     2.0|    60.0|    null|    null|    null|    null|    null|    null|    null|    null|    null|  null|   null|    null|  null|  null|   null|  null|   null|   null|  10.0|    1.0|                b''|         b''|   5.0|11011.0|28.781560195347467|     1.0|28.781560195347467|    null|   null|   null|    null|     9.0|              null|108.06090325480395|    1.0|     2.0|     1.0|     2.0|    1.0|   2.0|     1.0|     1.0|     3.0|     2.0|    1.0|    1.0|     2.0|  1.0|     1.0|     1.0|     1.0|     7.0|     1.0|  52.0|   4.0| 68.0|173.0| 7484.0|2509.0|     3.0|    2.0|     1.0|    4.0|    1.0|     1.0|     2.0|     2.0|5.397605346934028...|     1.0|5.397605346934028...|     1.0|                 7.0|    17.0|                 7.0|    29.0|    29.0|    13.0|5.397605346934028...|5.397605346934028...|                 1.0|                 1.0|    24.0|    78.0|    2.0|    2.0|   1.0|   1.0|5.397605346934028...|5.397605346934028...|     1.0|    35.0|5.397605346934028...|            2876.0|             493.0|     1.0|5.397605346934028...|   60.0|   60.0|  2800.0|  2800.0|   168.0|5.397605346934028...|5.397605346934028...|5.397605346934028...|   168.0|5.397605346934028...|  168.0|5.397605346934028...|5.397605346934028...|5.397605346934028...|    2.0|     1.0|     1.0|     2.0|     2.0|     2.0|    2.0|     2.0|     3.0|     3.0|     4.0|     2.0|     2.0|    null|    null|     2.0|\n",
      "|   1.0|   1.0|b'02012015'| b'02'|b'01'|b'2015'|  1200.0|2.015000003E9|2.015000003E9|     1.0|     1.0|    null|     1.0|     2.0|  null|     2.0|                 1.0|     1.0|    null|    null|  null|    null|    null|  null|    null|   null|    4.0|    15.0|    88.0|    88.0|     1.0|     2.0|    2.0|     1.0|    3.0|  null|     1.0|    1.0|    1.0|     7.0|     2.0|     1.0|    2.0|   null|     2.0|     1.0|     2.0|     1.0|     2.0|     2.0|     3.0|    null|2.0|    2.0|  4.0|     1.0|     2.0|    null|    1.0|     2.0|    7.0|    88.0|   99.0|     2.0|  158.0|  511.0|    null|     2.0|     2.0|  2.0|   2.0|    null|    null|    null|    null|    null|    null|    null|   null|   null|    null|    null|    null|    null|  null|   null|   null|   null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|   null|    null|    null|     1.0|     3.0|   null|    null|    null|    null|    null|   null|   null|   null|   null|    null|    null|    null|   null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|   null|    null|  null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|   null|    null|   null|   null|    null|    null|    null|  null|   null|   null|    null|   null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|   null|    null|    null|     b''|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|  null|   null|    null|  null|  null|   null|  null|   null|   null|  10.0|    1.0|                b''|         b''|   5.0|11011.0|28.781560195347467|     2.0|57.563120390694934|    null|   null|   null|    null|     1.0|0.6141246821308681|255.26479650234717|    2.0|     9.0|     1.0|     1.0|    2.0|  null|     1.0|     1.0|     3.0|     1.0|    1.0|    1.0|     2.0|  1.0|     1.0|     1.0|     1.0|    11.0|     2.0|  71.0|   6.0| 71.0|180.0| 7167.0|2204.0|     2.0|    1.0|     1.0|    2.0|    9.0|     9.0|     9.0|     9.0|               900.0|     9.0|             99900.0|     9.0|                null|    null|                null|    null|    null|    null|                 2.0|                 4.0|5.397605346934028...|5.397605346934028...|    null|    null|    9.0|    9.0|   1.0|   1.0|                 1.0|                 1.0|     9.0|    null|                null|            2173.0|             373.0|    null|                null|   null|   null|    null|    null|    null|                null|                null|                 9.0|    null|                null|   null|                null|                null|                null|    9.0|     9.0|     9.0|     9.0|     9.0|     9.0|    9.0|     9.0|     9.0|     9.0|     9.0|     9.0|     9.0|     9.0|     9.0|    null|\n",
      "|   1.0|   1.0|b'01142015'| b'01'|b'14'|b'2015'|  1100.0|2.015000004E9|2.015000004E9|     1.0|     1.0|    null|     1.0|     2.0|  null|     3.0|                 1.0|     2.0|    null|    null|  null|    null|    null|  null|    null|   null|    5.0|    30.0|    30.0|    30.0|     1.0|     2.0|    1.0|     1.0|    1.0|   1.0|     1.0|    1.0|    1.0|     2.0|     2.0|     2.0|    2.0|   null|     2.0|     1.0|     2.0|     1.0|     1.0|     2.0|     3.0|    null|2.0|    1.0|  4.0|     1.0|     2.0|    null|    1.0|     2.0|    8.0|     1.0|    8.0|     2.0|  180.0|  507.0|    null|     1.0|     2.0|  1.0|   1.0|     1.0|     2.0|     1.0|     2.0|    null|    null|    null|    3.0|  888.0|    null|    null|    null|   555.0| 101.0|  555.0|  301.0|  301.0|   201.0|     2.0|    null|    null|    null|    null|    null|    null|   888.0|     1.0|     1.0|     1.0|     8.0|     1.0|     1.0|777777.0|     5.0|     1.0|    9.0|    null|    null|     2.0|     3.0|   null|    null|    null|    null|    null|   null|   null|   null|   null|    null|    null|    null|   null|     2.0|    null|    null|    null|    null|    null|    null|    null|     2.0|    null|    null|    null|    null|    null|    null|    null|    null|    null|     1.0|    1.0|     1.0|   2.0|     1.0|     1.0|     2.0|    null|     2.0|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|   null|    null|   null|   null|    null|    null|    null|  null|   null|   null|    null|   null|    null|    null|    null|    null|     2.0|    null|     1.0|     2.0|     1.0|    null|    null|    null|    null|   null|    null|    null|     b''|     4.0|     7.0|    null|    null|    null|    97.0|    null|    null|    null|    null|    null|    null|    null|    null|    null|  null|   null|    null|  null|  null|   null|  null|   null|   null|  10.0|    1.0|                b''|         b''|   3.0|11011.0|28.781560195347467|     3.0|  86.3446805860424|    null|   null|   null|    null|     1.0|0.6141246821308681|341.38485273395895|    2.0|     1.0|     2.0|     1.0|    2.0|   2.0|     1.0|     1.0|     3.0|     1.0|    1.0|    1.0|     2.0|  1.0|     1.0|     1.0|     1.0|     9.0|     1.0|  63.0|   5.0| 67.0|170.0| 8165.0|2819.0|     3.0|    2.0|     2.0|    2.0|    5.0|     4.0|     1.0|     2.0|5.397605346934028...|     1.0|5.397605346934028...|     1.0|5.397605346934028...|   100.0|5.397605346934028...|     3.0|     3.0|    14.0|5.397605346934028...|5.397605346934028...|                 1.0|                 1.0|   100.0|    20.0|    1.0|    2.0|   1.0|   1.0|5.397605346934028...|5.397605346934028...|     2.0|    null|                null|            2469.0|423.00000000000006|    null|                null|   null|   null|    null|    null|    null|                null|5.397605346934028...|5.397605346934028...|    null|                null|   null|                null|                null|                null|    4.0|     2.0|     3.0|     3.0|     2.0|     2.0|    4.0|     2.0|     1.0|     1.0|     1.0|     1.0|     1.0|    null|    null|     9.0|\n",
      "|   1.0|   1.0|b'01142015'| b'01'|b'14'|b'2015'|  1100.0|2.015000005E9|2.015000005E9|     1.0|     1.0|    null|     1.0|     2.0|  null|     2.0|                 1.0|     1.0|    null|    null|  null|    null|    null|  null|    null|   null|    5.0|    20.0|    88.0|    30.0|     1.0|     1.0|    2.0|     1.0|    3.0|  null|     1.0|    1.0|    2.0|     2.0|     2.0|     2.0|    2.0|   null|     2.0|     2.0|     2.0|     1.0|     2.0|     2.0|     3.0|    null|2.0|    1.0|  5.0|     1.0|     2.0|    null|    2.0|     2.0|    8.0|    88.0|   77.0|     1.0|  142.0|  504.0|    null|     2.0|     2.0|  2.0|   2.0|     2.0|     2.0|     2.0|     2.0|    null|    null|    null|    3.0|  888.0|    null|    null|    null|   777.0| 102.0|  203.0|  204.0|  310.0|   320.0|     2.0|    null|    null|    null|    null|    null|    null|   888.0|     1.0|     1.0|     1.0|     7.0|     1.0|     2.0|    null|    null|     1.0|    1.0|777777.0|     1.0|     1.0|     3.0|   null|    null|    null|    null|    null|   null|   null|   null|   null|    null|    null|    null|   null|     2.0|    null|    null|    null|    null|    null|    null|    null|     7.0|    null|    null|    null|    null|    null|    null|    null|    null|    null|     2.0|   null|    null|  null|    null|    null|     1.0|   777.0|     2.0|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|   null|    null|   null|   null|    null|    null|    null|  null|   null|   null|    null|   null|    null|    null|    null|    null|     2.0|    null|     1.0|     2.0|     5.0|    null|    null|    null|    null|   null|    null|    null|     b''|     5.0|     5.0|    null|    null|    null|    45.0|    null|    null|    null|    null|    null|    null|    null|    null|    null|  null|   null|    null|  null|  null|   null|  null|   null|   null|  10.0|    1.0|                b''|         b''|   3.0|11011.0|28.781560195347467|     2.0|57.563120390694934|    null|   null|   null|    null|     9.0|              null|258.68222303399165|    2.0|     1.0|     1.0|     1.0|    1.0|   2.0|     1.0|     1.0|     3.0|     1.0|    1.0|    1.0|     2.0|  1.0|     1.0|     1.0|     1.0|     9.0|     1.0|  61.0|   5.0| 64.0|163.0| 6441.0|2437.0|     2.0|    1.0|     1.0|    3.0|    9.0|     4.0|     1.0|     2.0|5.397605346934028...|     1.0|5.397605346934028...|     1.0|                null|   200.0|                43.0|    57.0|    33.0|    67.0|                 1.0|5.397605346934028...|5.397605346934028...|                 1.0|    null|   200.0|    9.0|    1.0|   1.0|   1.0|                 1.0|5.397605346934028...|     2.0|    null|                null|            2543.0|436.00000000000006|    null|                null|   null|   null|    null|    null|    null|                null|5.397605346934028...|5.397605346934028...|    null|                null|   null|                null|                null|                null|    4.0|     2.0|     3.0|     3.0|     2.0|     2.0|    4.0|     2.0|     1.0|     1.0|     1.0|     1.0|     1.0|    null|    null|     1.0|\n",
      "|   1.0|   1.0|b'01142015'| b'01'|b'14'|b'2015'|  1100.0|2.015000006E9|2.015000006E9|     1.0|     1.0|    null|     1.0|     2.0|  null|     1.0|5.397605346934028...|     1.0|    null|    null|  null|    null|    null|  null|    null|   null|    2.0|    88.0|    88.0|    null|     1.0|     1.0|    2.0|     1.0|    1.0|   1.0|     1.0|    1.0|    2.0|     2.0|     2.0|     2.0|    2.0|   null|     2.0|     2.0|     2.0|     1.0|     2.0|     2.0|     3.0|    null|2.0|    3.0|  3.0|     1.0|     2.0|    null|    2.0|     2.0|    2.0|    88.0|    6.0|     2.0|  145.0|  502.0|    null|     1.0|     2.0|  2.0|   2.0|     2.0|     2.0|     2.0|     2.0|    null|    null|    null|    3.0|  888.0|    null|    null|    null|   101.0| 101.0|  102.0|  101.0|  102.0|   101.0|     1.0|    18.0|   101.0|   100.0|    73.0|   107.0|    30.0|   888.0|     1.0|     2.0|     3.0|     4.0|     1.0|     1.0|112014.0|     1.0|     1.0|    2.0|    null|    null|     1.0|     3.0|   null|    null|    null|    null|    null|   null|   null|   null|   null|    null|    null|    null|   null|     2.0|    null|    null|    null|    null|    null|    null|    null|     2.0|    null|    null|    null|    null|    null|    null|    null|    null|    null|     2.0|   null|    null|  null|    null|    null|     1.0|   415.0|     2.0|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|   null|    null|   null|   null|    null|    null|    null|  null|   null|   null|    null|   null|    null|    null|    null|    null|     2.0|    null|     1.0|     2.0|     1.0|    null|    null|    null|    null|   null|    null|    null|     b''|     5.0|     5.0|     1.0|    54.0|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|  null|   null|    null|  null|  null|   null|  null|   null|   null|  10.0|    1.0|                b''|         b''|   5.0|11011.0|28.781560195347467|     1.0|28.781560195347467|    null|   null|   null|    null|     9.0|              null|256.51859133319607|    1.0|     9.0|     2.0|     1.0|    1.0|   2.0|     1.0|     1.0|     3.0|     1.0|    1.0|    1.0|     2.0|  1.0|     1.0|     1.0|     1.0|    11.0|     2.0|  73.0|   6.0| 62.0|157.0| 6577.0|2652.0|     3.0|    2.0|     1.0|    1.0|    4.0|     4.0|     1.0|     2.0|5.397605346934028...|     1.0|5.397605346934028...|     1.0|               100.0|   100.0|               200.0|   100.0|   200.0|   100.0|5.397605346934028...|5.397605346934028...|                 1.0|                 1.0|   200.0|   600.0|    1.0|    1.0|   1.0|   1.0|5.397605346934028...|5.397605346934028...|     1.0|    50.0|                33.0|            2099.0|             360.0|     2.0|                 1.0|   60.0|   30.0|  1000.0|  7000.0|    60.0|               210.0|5.397605346934028...|5.397605346934028...|   120.0|               210.0|  330.0|                60.0|5.397605346934028...|                60.0|    1.0|     1.0|     1.0|     1.0|     1.0|     2.0|    2.0|     2.0|     1.0|     2.0|     3.0|     1.0|     1.0|     1.0|     1.0|     2.0|\n",
      "|   1.0|   1.0|b'01052015'| b'01'|b'05'|b'2015'|  1100.0|2.015000007E9|2.015000007E9|     1.0|     1.0|    null|     1.0|     2.0|  null|     1.0|5.397605346934028...|     1.0|    null|    null|  null|    null|    null|  null|    null|   null|    2.0|    88.0|     3.0|    88.0|     1.0|     1.0|    2.0|     1.0|    1.0|   1.0|     1.0|    1.0|    1.0|     2.0|     2.0|     2.0|    2.0|   null|     2.0|     2.0|     2.0|     2.0|     2.0|     2.0|     3.0|    null|2.0|    3.0|  5.0|     1.0|     2.0|    null|    2.0|     2.0|    7.0|    88.0|    4.0|     2.0|  148.0|  506.0|    null|     2.0|     2.0|  2.0|   2.0|     2.0|     2.0|     2.0|     2.0|    null|    null|    null|    3.0|  203.0|     1.0|    88.0|     1.0|   330.0| 202.0|  302.0|  202.0|  302.0|   205.0|     1.0|    18.0|   102.0|   100.0|    64.0|   107.0|    15.0|   888.0|    null|    null|    null|    null|     1.0|     2.0|    null|    null|     1.0|    1.0| 91991.0|     4.0|     2.0|     3.0|   null|    null|    null|    null|    null|   null|   null|   null|   null|    null|    null|    null|   null|     8.0|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|     2.0|   null|    null|  null|    null|    null|     1.0|   403.0|     1.0|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|   null|    null|   null|   null|    null|    null|    null|  null|   null|   null|    null|   null|    null|    null|    null|    null|     1.0|     5.0|     1.0|     1.0|     6.0|    null|    null|    null|    null|   null|    null|    null|     b''|     4.0|     5.0|    null|    null|     2.0|    40.0|    null|    null|    null|    null|    null|    null|    null|    null|    null|  null|   null|    null|  null|  null|   null|  null|   null|   null|  10.0|    1.0|                b''|         b''|   1.0|11011.0|28.781560195347467|     1.0|28.781560195347467|    null|   null|   null|    null|     9.0|              null| 85.65975469444061|    1.0|     9.0|     2.0|     1.0|    2.0|   2.0|     1.0|     1.0|     3.0|     2.0|    1.0|    7.0|     2.0|  7.0|     2.0|     4.0|     5.0|    11.0|     2.0|  70.0|   6.0| 66.0|168.0| 6713.0|2389.0|     2.0|    1.0|     1.0|    3.0|    2.0|     4.0|     1.0|     1.0|                10.0|     1.0|                70.0|     1.0|               100.0|    29.0|                 7.0|    29.0|     7.0|    71.0|5.397605346934028...|5.397605346934028...|                 1.0|                 1.0|   129.0|   114.0|    1.0|    1.0|   1.0|   1.0|5.397605346934028...|5.397605346934028...|     1.0|    50.0|                35.0|            2210.0|             379.0|     2.0|                 1.0|   60.0|   15.0|  2000.0|  7000.0|   120.0|               105.0|5.397605346934028...|5.397605346934028...|   240.0|               105.0|  345.0|               120.0|5.397605346934028...|               120.0|    1.0|     1.0|     1.0|     1.0|     1.0|     2.0|    2.0|     2.0|     3.0|     3.0|     4.0|     1.0|     1.0|     2.0|     1.0|     1.0|\n",
      "|   1.0|   1.0|b'01142015'| b'01'|b'14'|b'2015'|  1100.0|2.015000008E9|2.015000008E9|     1.0|     1.0|    null|     1.0|     2.0|  null|     2.0|                 1.0|     1.0|    null|    null|  null|    null|    null|  null|    null|   null|    5.0|     8.0|    88.0|     8.0|     1.0|     1.0|    1.0|     1.0|    1.0|   1.0|     1.0|    1.0|    1.0|     7.0|     2.0|     2.0|    2.0|   null|     2.0|     2.0|     2.0|     1.0|     2.0|     7.0|     3.0|    null|1.0|    1.0|  3.0|     1.0|     2.0|    null|    1.0|     1.0|    3.0|    88.0|    3.0|     2.0|  179.0|  501.0|    null|     1.0|     2.0|  2.0|   2.0|     1.0|     2.0|     2.0|     1.0|     3.0|    null|     7.0|    3.0|  888.0|    null|    null|    null|   102.0| 101.0|  202.0|  101.0|  303.0|   202.0|     1.0|    64.0|   106.0|    12.0|    98.0|   107.0|     5.0|   888.0|     1.0|     1.0|     1.0|    77.0|     1.0|     1.0|122014.0|     1.0|     1.0|    1.0|777777.0|     4.0|     1.0|     3.0|   null|    null|    null|    null|    null|   null|   null|   null|   null|    null|    null|    null|   null|     2.0|    null|    null|    null|    null|    null|    null|    null|     2.0|    null|    null|    null|    null|    null|    null|    null|    null|    null|     2.0|   null|    null|  null|    null|    null|     1.0|   777.0|     2.0|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|   null|    null|   null|   null|    null|    null|    null|  null|   null|   null|    null|   null|    null|    null|    null|    null|     2.0|    null|     1.0|     2.0|     4.0|    null|    null|    null|    null|   null|    null|    null|     b''|     4.0|     2.0|    null|    null|     1.0|    45.0|    null|    null|    null|    null|    null|    null|    null|    null|    null|  null|   null|    null|  null|  null|   null|  null|   null|   null|  10.0|    1.0|                b''|b'Treadmill'|   5.0|11011.0|28.781560195347467|     2.0|57.563120390694934|    null|   null|   null|    null|     1.0|0.6141246821308681| 545.7820952341142|    2.0|     9.0|     2.0|     1.0|    2.0|  null|     1.0|     1.0|     3.0|     1.0|    1.0|    1.0|     2.0|  1.0|     1.0|     1.0|     1.0|    13.0|     2.0|  80.0|   6.0| 61.0|155.0| 8119.0|3382.0|     4.0|    2.0|     1.0|    1.0|    2.0|     3.0|     1.0|     2.0|5.397605346934028...|     1.0|5.397605346934028...|     1.0|               200.0|   100.0|                29.0|   100.0|    10.0|    29.0|5.397605346934028...|5.397605346934028...|                 1.0|                 1.0|   300.0|   168.0|    1.0|    1.0|   1.0|   1.0|5.397605346934028...|5.397605346934028...|     1.0|    35.0|                45.0|            1545.0|             265.0|     2.0|                 2.0|   12.0|    5.0|  6000.0|  7000.0|    72.0|5.397605346934028...|5.397605346934028...|5.397605346934028...|   144.0|5.397605346934028...|  144.0|                72.0|5.397605346934028...|                72.0|    3.0|     2.0|     2.0|     2.0|     2.0|     2.0|    4.0|     2.0|     1.0|     1.0|     1.0|     1.0|     1.0|     1.0|     1.0|     1.0|\n",
      "|   1.0|   1.0|b'01132015'| b'01'|b'13'|b'2015'|  1100.0|2.015000009E9|2.015000009E9|     1.0|     1.0|    null|     1.0|     2.0|  null|     1.0|5.397605346934028...|     1.0|    null|    null|  null|    null|    null|  null|    null|   null|    5.0|    77.0|    88.0|    77.0|     1.0|     1.0|    2.0|     1.0|    3.0|  null|     7.0|   null|   null|     2.0|     2.0|     2.0|    2.0|   null|     2.0|     2.0|     2.0|     1.0|     2.0|     2.0|     3.0|    null|2.0|    3.0|  3.0|     1.0|     2.0|    null|    1.0|     2.0|    5.0|    88.0|   77.0|     2.0|   84.0|  503.0|    null|     1.0|     1.0|  2.0|   2.0|     7.0|     2.0|     2.0|     2.0|    null|    null|    null|    3.0|  888.0|    null|    null|    null|   777.0| 777.0|  302.0|  302.0|  777.0|   777.0|     1.0|    98.0|   103.0|   100.0|    88.0|    null|    null|   777.0|     2.0|     1.0|     2.0|     6.0|     1.0|     1.0|777777.0|     1.0|     1.0|    2.0|    null|    null|     1.0|     3.0|   null|    null|    null|    null|    null|   null|   null|   null|   null|    null|    null|    null|   null|     2.0|    null|    null|    null|    null|    null|    null|    null|     2.0|    null|    null|    null|    null|    null|    null|    null|    null|    null|     2.0|   null|    null|  null|    null|    null|     7.0|    null|     1.0|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|   null|    null|   null|   null|    null|    null|    null|  null|   null|   null|    null|   null|    null|    null|    null|    null|     2.0|    null|     2.0|    null|    null|    null|    null|    null|    null|   null|    null|    null|     b''|     5.0|     5.0|    null|    null|    null|    98.0|    null|    null|    null|    null|    null|    null|    null|    null|    null|  null|   null|    null|  null|  null|   null|  null|   null|   null|  10.0|    1.0|b'Physical Therapy'|         b''|   1.0|11011.0|28.781560195347467|     1.0|28.781560195347467|    null|   null|   null|    null|     1.0|0.6141246821308681|211.21029542946906|    2.0|     9.0|     1.0|     9.0|   null|   2.0|     1.0|     1.0|     3.0|     1.0|    1.0|    1.0|     2.0|  1.0|     1.0|     1.0|     1.0|    13.0|     2.0|  80.0|   6.0| 63.0|160.0| 3810.0|1488.0|     1.0|    1.0|     1.0|    1.0|    9.0|     4.0|     1.0|     2.0|5.397605346934028...|     1.0|5.397605346934028...|     1.0|                null|    null|                 7.0|     7.0|    null|    null|                 2.0|                 2.0|5.397605346934028...|5.397605346934028...|    null|    null|    9.0|    9.0|   1.0|   1.0|                 1.0|                 1.0|     1.0|    45.0|5.397605346934028...|            1618.0|             277.0|     2.0|5.397605346934028...|   60.0|   null|  3000.0|    null|   180.0|5.397605346934028...|                null|5.397605346934028...|   360.0|5.397605346934028...|  360.0|               180.0|5.397605346934028...|               180.0|    1.0|     1.0|     1.0|     1.0|     1.0|     9.0|    9.0|     9.0|     2.0|     1.0|     2.0|     1.0|     1.0|     1.0|     1.0|     2.0|\n",
      "|   1.0|   1.0|b'01302015'| b'01'|b'30'|b'2015'|  1100.0| 2.01500001E9| 2.01500001E9|     1.0|     1.0|    null|     1.0|     2.0|  null|     2.0|                 1.0|     1.0|    null|    null|  null|    null|    null|  null|    null|   null|    2.0|     2.0|    88.0|     2.0|     1.0|     1.0|    2.0|     1.0|    1.0|   1.0|     1.0|    1.0|    1.0|     2.0|     2.0|     2.0|    2.0|   null|     2.0|     2.0|     2.0|     2.0|     2.0|     2.0|     3.0|    null|1.0|    1.0|  6.0|     1.0|     2.0|    null|    1.0|     2.0|    7.0|    88.0|    8.0|     1.0|  161.0|  507.0|    null|     2.0|     2.0|  2.0|   2.0|     2.0|     2.0|     2.0|     1.0|     3.0|    null|     7.0|    3.0|  888.0|    null|    null|    null|   201.0| 101.0|  204.0|  205.0|  206.0|   101.0|     1.0|    64.0|   106.0|    50.0|    88.0|    null|    null|   888.0|    null|    null|    null|    null|     2.0|     2.0|    null|    null|     2.0|    2.0|    null|    null|     1.0|     3.0|   null|    null|    null|    null|    null|   null|   null|   null|   null|    null|    null|    null|   null|     2.0|    null|    null|    null|    null|    null|    null|    null|     2.0|    null|    null|    null|    null|    null|    null|    null|    null|    null|     2.0|   null|    null|  null|    null|    null|     1.0|   415.0|     1.0|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|   null|    null|   null|   null|    null|    null|    null|  null|   null|   null|    null|   null|    null|    null|    null|    null|     2.0|    null|     1.0|     2.0|     2.0|    null|    null|    null|    null|   null|    null|    null|     b''|     5.0|     5.0|    null|    null|     1.0|    60.0|    null|    null|    null|    null|    null|    null|    null|    null|    null|  null|   null|    null|  null|  null|   null|  null|   null|   null|  10.0|    1.0|                b''|         b''|   1.0|11011.0|28.781560195347467|     2.0|57.563120390694934|    null|   null|   null|    null|     1.0|0.6141246821308681|215.47286326207018|    1.0|     9.0|     2.0|     1.0|    2.0|   2.0|     1.0|     1.0|     3.0|     2.0|    1.0|    1.0|     2.0|  1.0|     1.0|     1.0|     1.0|    10.0|     2.0|  68.0|   6.0| 67.0|170.0| 7303.0|2522.0|     3.0|    2.0|     1.0|    4.0|    5.0|     3.0|     1.0|     2.0|5.397605346934028...|     1.0|5.397605346934028...|     1.0|                14.0|   100.0|                57.0|    71.0|    86.0|   100.0|5.397605346934028...|5.397605346934028...|                 1.0|                 1.0|   114.0|   314.0|    1.0|    1.0|   1.0|   1.0|5.397605346934028...|5.397605346934028...|     1.0|    35.0|5.397605346934028...|            2260.0|             387.0|     1.0|5.397605346934028...|   50.0|   null|  6000.0|    null|   300.0|5.397605346934028...|5.397605346934028...|5.397605346934028...|   300.0|5.397605346934028...|  300.0|5.397605346934028...|5.397605346934028...|5.397605346934028...|    2.0|     1.0|     1.0|     2.0|     2.0|     2.0|    2.0|     2.0|     3.0|     3.0|     4.0|     1.0|     2.0|     2.0|     2.0|     2.0|\n",
      "|   1.0|   1.0|b'01222015'| b'01'|b'22'|b'2015'|  1100.0|2.015000011E9|2.015000011E9|     1.0|     1.0|    null|     1.0|     2.0|  null|     2.0|                 1.0|     1.0|    null|    null|  null|    null|    null|  null|    null|   null|    3.0|    14.0|    88.0|    14.0|     1.0|     1.0|    2.0|     1.0|    1.0|   1.0|     1.0|    1.0|    2.0|     2.0|     2.0|     2.0|    2.0|   null|     2.0|     2.0|     2.0|     2.0|     2.0|     2.0|     3.0|    null|2.0|    1.0|  6.0|     1.0|     2.0|    null|    1.0|     2.0|    7.0|    88.0|    7.0|     1.0|  175.0|  504.0|    null|     2.0|     2.0|  2.0|   2.0|     2.0|     2.0|     2.0|     1.0|     3.0|    null|    77.0|    3.0|  888.0|    null|    null|    null|   202.0| 201.0|  303.0|  202.0|  201.0|   203.0|     2.0|    null|    null|    null|    null|    null|    null|   888.0|    null|    null|    null|    null|     1.0|     1.0| 92014.0|     1.0|     1.0|    2.0|    null|    null|     1.0|     3.0|   null|    null|    null|    null|    null|   null|   null|   null|   null|    null|    null|    null|   null|     2.0|    null|    null|    null|    null|    null|    null|    null|     2.0|    null|    null|    null|    null|    null|    null|    null|    null|    null|     2.0|   null|    null|  null|    null|    null|     1.0|   405.0|     1.0|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|   null|    null|   null|   null|    null|    null|    null|  null|   null|   null|    null|   null|    null|    null|    null|    null|     1.0|     2.0|     1.0|     2.0|     2.0|    null|    null|    null|    null|   null|    null|    null|     b''|     5.0|     5.0|    null|    null|     1.0|    40.0|    null|    null|    null|    null|    null|    null|    null|    null|    null|  null|   null|    null|  null|  null|   null|  null|   null|   null|  10.0|    1.0|                b''|         b''|   1.0|11011.0|28.781560195347467|     2.0|57.563120390694934|    null|   null|   null|    null|     1.0|0.6141246821308681|136.14486420629444|    1.0|     1.0|     2.0|     1.0|    1.0|   2.0|     1.0|     1.0|     3.0|     2.0|    1.0|    1.0|     2.0|  1.0|     1.0|     1.0|     1.0|     9.0|     1.0|  62.0|   5.0| 64.0|163.0| 7938.0|3004.0|     4.0|    2.0|     1.0|    4.0|    5.0|     3.0|     1.0|     2.0|5.397605346934028...|     1.0|5.397605346934028...|     1.0|                29.0|    14.0|                10.0|    29.0|    14.0|    43.0|5.397605346934028...|5.397605346934028...|                 1.0|                 1.0|    43.0|    96.0|    2.0|    2.0|   1.0|   1.0|5.397605346934028...|5.397605346934028...|     2.0|    null|                null|            2506.0|             430.0|    null|                null|   null|   null|    null|    null|    null|                null|5.397605346934028...|5.397605346934028...|    null|                null|   null|                null|                null|                null|    4.0|     2.0|     3.0|     3.0|     2.0|     2.0|    4.0|     2.0|     3.0|     3.0|     4.0|     1.0|     1.0|    null|    null|     2.0|\n",
      "|   1.0|   1.0|b'01162015'| b'01'|b'16'|b'2015'|  1100.0|2.015000012E9|2.015000012E9|     1.0|     1.0|    null|     1.0|     2.0|  null|     1.0|5.397605346934028...|     1.0|    null|    null|  null|    null|    null|  null|    null|   null|    5.0|    88.0|    88.0|    null|     1.0|     1.0|    1.0|     1.0|    1.0|   1.0|     1.0|    1.0|    1.0|     2.0|     1.0|     1.0|    2.0|   null|     1.0|     1.0|     1.0|     1.0|     2.0|     2.0|     1.0|    98.0|2.0|    3.0|  5.0|     1.0|     2.0|    null|    1.0|     2.0|    8.0|    88.0|   77.0|     2.0|  150.0|  504.0|    null|     1.0|     1.0|  1.0|   1.0|     1.0|     1.0|     1.0|     2.0|    null|    null|    null|    3.0|  888.0|    null|    null|    null|   203.0| 203.0|  201.0|  304.0|  205.0|   306.0|     2.0|    null|    null|    null|    null|    null|    null|   888.0|     1.0|     1.0|     1.0|     9.0|     1.0|     1.0| 92014.0|     1.0|     1.0|    2.0|    null|    null|    null|    null|   null|    null|    null|    null|    null|   null|   null|   null|   null|    null|    null|    null|   null|     2.0|    null|    null|    null|    null|    null|    null|    null|     2.0|    null|    null|    null|    null|    null|    null|    null|    null|    null|     1.0|    7.0|     1.0|   1.0|     1.0|     1.0|     2.0|    null|     2.0|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|   null|    null|   null|   null|    null|    null|    null|  null|   null|   null|    null|   null|    null|    null|    null|    null|     1.0|     1.0|     1.0|     2.0|     1.0|    null|    null|    null|    null|   null|    null|    null|     b''|     3.0|     5.0|    null|    null|    null|    97.0|    null|    null|    null|    null|    null|    null|    null|    null|    null|  null|   null|    null|  null|  null|   null|  null|   null|   null|  10.0|    1.0|                b''|         b''|   2.0|11011.0|28.781560195347467|     1.0|28.781560195347467|    null|   null|   null|    null|     1.0|0.6141246821308681|119.44576700017332|    2.0|     9.0|     2.0|     1.0|    2.0|   1.0|     1.0|     1.0|     3.0|     1.0|    1.0|    1.0|     2.0|  1.0|     1.0|     1.0|     1.0|    13.0|     2.0|  80.0|   6.0| 64.0|163.0| 6804.0|2575.0|     3.0|    2.0|     1.0|    3.0|    9.0|     4.0|     1.0|     2.0|5.397605346934028...|     1.0|5.397605346934028...|     1.0|                43.0|    43.0|                14.0|    13.0|    71.0|    20.0|5.397605346934028...|5.397605346934028...|                 1.0|                 1.0|    86.0|   118.0|    2.0|    1.0|   1.0|   1.0|5.397605346934028...|5.397605346934028...|     2.0|    null|                null|            1618.0|             277.0|    null|                null|   null|   null|    null|    null|    null|                null|5.397605346934028...|5.397605346934028...|    null|                null|   null|                null|                null|                null|    4.0|     2.0|     3.0|     3.0|     2.0|     2.0|    4.0|     2.0|     1.0|     1.0|     1.0|     1.0|     1.0|     1.0|     1.0|     2.0|\n",
      "|   1.0|   1.0|b'01202015'| b'01'|b'20'|b'2015'|  1200.0|2.015000013E9|2.015000013E9|     1.0|     1.0|    null|     1.0|     2.0|  null|     1.0|5.397605346934028...|     1.0|    null|    null|  null|    null|    null|  null|    null|   null|    5.0|     5.0|    88.0|    88.0|     1.0|     1.0|    2.0|     1.0|    1.0|   1.0|     1.0|    1.0|    1.0|     2.0|     2.0|     1.0|    1.0|    2.0|     1.0|     1.0|     2.0|     1.0|     2.0|     2.0|     1.0|    60.0|2.0|    1.0|  5.0|     1.0|     2.0|    null|    1.0|     2.0|    7.0|    88.0|    5.0|     1.0| 9999.0|  501.0|    null|     1.0|     1.0|  2.0|   2.0|     1.0|     2.0|     1.0|     1.0|     3.0|    null|     7.0|    3.0|  888.0|    null|    null|    null|    null|  null|   null|   null|   null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|   null|    null|    null|    null|    null|   null|    null|    null|    null|    null|   null|   null|   null|   null|    null|    null|    null|   null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|   null|    null|  null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|   null|    null|   null|   null|    null|    null|    null|  null|   null|   null|    null|   null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|   null|    null|    null|     b''|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|  null|   null|    null|  null|  null|   null|  null|   null|   null|  10.0|    1.0|                b''|         b''|   2.0|11011.0|28.781560195347467|     1.0|28.781560195347467|    null|   null|   null|    null|     1.0|0.6141246821308681|119.15766850259065|    2.0|     9.0|     2.0|     1.0|    2.0|   2.0|     2.0|     1.0|     2.0|     1.0|    1.0|    1.0|     2.0|  1.0|     1.0|     1.0|     1.0|    11.0|     2.0|  72.0|   6.0| 61.0|155.0|   null|  null|    null|    9.0|     1.0|    3.0|    3.0|     3.0|     1.0|     2.0|5.397605346934028...|     1.0|5.397605346934028...|     1.0|                null|    null|                null|    null|    null|    null|                 2.0|                 4.0|5.397605346934028...|5.397605346934028...|    null|    null|    9.0|    9.0|   1.0|   1.0|                 1.0|                 1.0|     9.0|    null|                null|            2136.0|             366.0|    null|                null|   null|   null|    null|    null|    null|                null|                null|                 9.0|    null|                null|   null|                null|                null|                null|    9.0|     9.0|     9.0|     9.0|     9.0|     9.0|    9.0|     9.0|     9.0|     9.0|     9.0|     9.0|     9.0|     9.0|     9.0|    null|\n",
      "|   1.0|   1.0|b'01202015'| b'01'|b'20'|b'2015'|  1200.0|2.015000014E9|2.015000014E9|     1.0|     1.0|    null|     1.0|     2.0|  null|     2.0|                 1.0|     1.0|    null|    null|  null|    null|    null|  null|    null|   null|    4.0|    30.0|    88.0|     5.0|     1.0|     1.0|    2.0|     1.0|    3.0|  null|     1.0|    1.0|    1.0|     2.0|     2.0|     2.0|    2.0|   null|     2.0|     2.0|     2.0|     1.0|     2.0|     2.0|     3.0|    null|2.0|    1.0|  5.0|     1.0|     2.0|    null|    1.0|     2.0|    7.0|    88.0|    7.0|     2.0|  140.0|  506.0|    null|     2.0|     2.0|  1.0|   2.0|     1.0|     1.0|     2.0|     2.0|    null|    null|    null|    3.0|  888.0|    null|    null|    null|   101.0| 101.0|  320.0|  305.0|   null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|   null|    null|    null|     1.0|     3.0|   null|    null|    null|    null|    null|   null|   null|   null|   null|    null|    null|    null|   null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|   null|    null|  null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|   null|    null|   null|   null|    null|    null|    null|  null|   null|   null|    null|   null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|   null|    null|    null|     b''|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|  null|   null|    null|  null|  null|   null|  null|   null|   null|  10.0|    1.0|                b''|         b''|   5.0|11011.0|28.781560195347467|     2.0|57.563120390694934|    null|   null|   null|    null|     1.0|0.6141246821308681|217.19107498091134|    2.0|     9.0|     1.0|     1.0|    2.0|   2.0|     1.0|     1.0|     3.0|     1.0|    1.0|    1.0|     2.0|  1.0|     1.0|     1.0|     1.0|    13.0|     2.0|  80.0|   6.0| 66.0|168.0| 6350.0|2260.0|     2.0|    1.0|     1.0|    3.0|    5.0|     4.0|     1.0|     2.0|5.397605346934028...|     1.0|5.397605346934028...|     1.0|               100.0|   100.0|                67.0|    17.0|    null|    null|5.397605346934028...|                 2.0|                 1.0|5.397605346934028...|   200.0|    null|    1.0|    9.0|   1.0|   1.0|5.397605346934028...|                 1.0|     9.0|    null|                null|            1729.0|             296.0|    null|                null|   null|   null|    null|    null|    null|                null|                null|                 9.0|    null|                null|   null|                null|                null|                null|    9.0|     9.0|     9.0|     9.0|     9.0|     9.0|    9.0|     9.0|     9.0|     9.0|     9.0|     9.0|     9.0|     9.0|     9.0|    null|\n",
      "|   1.0|   1.0|b'01202015'| b'01'|b'20'|b'2015'|  1200.0|2.015000015E9|2.015000015E9|     1.0|     1.0|    null|     1.0|     2.0|  null|     1.0|5.397605346934028...|     1.0|    null|    null|  null|    null|    null|  null|    null|   null|    3.0|    88.0|    88.0|    null|     1.0|     2.0|    2.0|     1.0|    1.0|   1.0|     1.0|    1.0|    2.0|     2.0|     1.0|     2.0|    2.0|   null|     2.0|     2.0|     2.0|     1.0|     2.0|     2.0|     3.0|    null|2.0|    3.0|  5.0|     2.0|     2.0|    null|    2.0|     2.0|    5.0|    88.0|   77.0|     2.0|  170.0|  507.0|    null|     1.0|     1.0|  2.0|   1.0|     1.0|     2.0|     1.0|     2.0|    null|    null|    null|    3.0|  888.0|    null|    null|    null|    null|  null|   null|   null|   null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|   null|    null|    null|     7.0|     3.0|   null|    null|    null|    null|    null|   null|   null|   null|   null|    null|    null|    null|   null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|   null|    null|  null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|   null|    null|   null|   null|    null|    null|    null|  null|   null|   null|    null|   null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|   null|    null|    null|     b''|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|  null|   null|    null|  null|  null|   null|  null|   null|   null|  10.0|    1.0|                b''|         b''|   1.0|11011.0|28.781560195347467|     1.0|28.781560195347467|    null|   null|   null|    null|     9.0|              null| 73.48576749547475|    1.0|     9.0|     2.0|     1.0|    1.0|   1.0|     1.0|     1.0|     3.0|     1.0|    1.0|    1.0|     2.0|  1.0|     1.0|     1.0|     1.0|    13.0|     2.0|  80.0|   6.0| 67.0|170.0| 7711.0|2663.0|     3.0|    2.0|     1.0|    3.0|    9.0|     4.0|     1.0|     2.0|5.397605346934028...|     1.0|5.397605346934028...|     1.0|                null|    null|                null|    null|    null|    null|                 2.0|                 4.0|5.397605346934028...|5.397605346934028...|    null|    null|    9.0|    9.0|   1.0|   1.0|                 1.0|                 1.0|     9.0|    null|                null|            1507.0|             258.0|    null|                null|   null|   null|    null|    null|    null|                null|                null|                 9.0|    null|                null|   null|                null|                null|                null|    9.0|     9.0|     9.0|     9.0|     9.0|     9.0|    9.0|     9.0|     9.0|     9.0|     9.0|     9.0|     9.0|     9.0|     9.0|    null|\n",
      "|   1.0|   1.0|b'01142015'| b'01'|b'14'|b'2015'|  1100.0|2.015000016E9|2.015000016E9|     1.0|     1.0|    null|     1.0|     2.0|  null|     1.0|5.397605346934028...|     1.0|    null|    null|  null|    null|    null|  null|    null|   null|    3.0|    88.0|    88.0|    null|     1.0|     1.0|    2.0|     1.0|    1.0|   1.0|     1.0|    1.0|    1.0|     2.0|     2.0|     2.0|    1.0|    1.0|     1.0|     2.0|     1.0|     1.0|     2.0|     2.0|     3.0|    null|2.0|    3.0|  4.0|     1.0|     2.0|    null|    1.0|     2.0|    5.0|    88.0|    4.0|     1.0|  128.0|  500.0|    null|     1.0|     2.0|  2.0|   2.0|     1.0|     2.0|     2.0|     1.0|     1.0|     1.0|    null|    3.0|  202.0|     2.0|    88.0|     2.0|   302.0| 308.0|  320.0|  320.0|  310.0|   330.0|     1.0|    64.0|   103.0|    20.0|    73.0|   101.0|   230.0|   888.0|     2.0|     2.0|     3.0|     4.0|     1.0|     1.0|122014.0|     1.0|     1.0|    2.0|    null|    null|     2.0|     3.0|   null|    null|    null|    null|    null|   null|   null|   null|   null|    null|    null|    null|   null|     1.0|    10.0|     4.0|     2.0|    10.0|     2.0|     1.0|     6.0|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|     2.0|   null|    null|  null|    null|    null|     1.0|   405.0|     1.0|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|   null|    null|   null|   null|    null|    null|    null|  null|   null|   null|    null|   null|    null|    null|    null|    null|     1.0|     2.0|     1.0|     2.0|     7.0|    null|    null|    null|    null|   null|    null|    null|     b''|     4.0|     4.0|    null|    null|    null|    97.0|    null|    null|    null|    null|    null|    null|    null|    null|    null|  null|   null|    null|  null|  null|   null|  null|   null|   null|  10.0|    1.0|                b''|         b''|   3.0|11011.0|28.781560195347467|     1.0|28.781560195347467|    null|   null|   null|    null|     1.0|0.6141246821308681|127.63239825117358|    1.0|     9.0|     2.0|     1.0|    2.0|   2.0|     2.0|     2.0|     1.0|     1.0|    1.0|    1.0|     2.0|  1.0|     1.0|     1.0|     1.0|    11.0|     2.0|  73.0|   6.0| 60.0|152.0| 5806.0|2500.0|     3.0|    2.0|     1.0|    2.0|    2.0|     1.0|     2.0|     1.0|                 7.0|     1.0|                93.0|     1.0|                 7.0|    27.0|                67.0|    67.0|    33.0|   100.0|5.397605346934028...|5.397605346934028...|                 1.0|                 1.0|    34.0|   267.0|    2.0|    1.0|   1.0|   1.0|5.397605346934028...|5.397605346934028...|     1.0|    35.0|                33.0|            2099.0|             360.0|     1.0|                 1.0|   20.0|  150.0|  3000.0|  1000.0|    60.0|               150.0|5.397605346934028...|5.397605346934028...|    60.0|               150.0|  210.0|5.397605346934028...|5.397605346934028...|5.397605346934028...|    2.0|     1.0|     1.0|     2.0|     2.0|     2.0|    2.0|     2.0|     2.0|     2.0|     3.0|     1.0|     1.0|     1.0|     1.0|     2.0|\n",
      "|   1.0|   1.0|b'01042015'| b'01'|b'04'|b'2015'|  1200.0|2.015000017E9|2.015000017E9|     1.0|     1.0|    null|     1.0|     2.0|  null|     1.0|5.397605346934028...|     1.0|    null|    null|  null|    null|    null|  null|    null|   null|    5.0|    30.0|    30.0|    30.0|     1.0|     1.0|    2.0|     1.0|    1.0|   1.0|     1.0|    1.0|    1.0|     1.0|     2.0|     2.0|    2.0|   null|     2.0|     1.0|     1.0|     1.0|     1.0|     2.0|     1.0|    41.0|2.0|    6.0|  5.0|     1.0|     2.0|    null|    2.0|     2.0|    8.0|    88.0|    1.0|     1.0|  200.0|  508.0|    null|     1.0|     2.0|  2.0|   1.0|     1.0|     1.0|     1.0|     1.0|     1.0|     2.0|    null|    3.0|  888.0|    null|    null|    null|   555.0| 330.0|  101.0|  302.0|  301.0|   101.0|     2.0|    null|    null|    null|    null|    null|    null|   888.0|     2.0|     2.0|     3.0|     5.0|     1.0|     1.0|122014.0|     1.0|     1.0|    9.0|    null|    null|    null|    null|   null|    null|    null|    null|    null|   null|   null|   null|   null|    null|    null|    null|   null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|   null|    null|  null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|   null|    null|   null|   null|    null|    null|    null|  null|   null|   null|    null|   null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|   null|    null|    null|     b''|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|  null|   null|    null|  null|  null|   null|  null|   null|   null|  10.0|    1.0|                b''|         b''|   3.0|11011.0|28.781560195347467|     1.0|28.781560195347467|    null|   null|   null|    null|     9.0|              null|156.79466713195177|    2.0|     1.0|     2.0|     1.0|    2.0|   1.0|     1.0|     1.0|     3.0|     1.0|    1.0|    1.0|     2.0|  1.0|     1.0|     1.0|     1.0|     9.0|     1.0|  63.0|   5.0| 68.0|173.0| 9072.0|3041.0|     4.0|    2.0|     1.0|    3.0|    1.0|     1.0|     2.0|     2.0|5.397605346934028...|     1.0|5.397605346934028...|     1.0|5.397605346934028...|   100.0|               100.0|     7.0|     3.0|   100.0|5.397605346934028...|5.397605346934028...|                 1.0|                 1.0|   100.0|   210.0|    1.0|    1.0|   1.0|   1.0|5.397605346934028...|5.397605346934028...|     2.0|    null|                null|            2469.0|423.00000000000006|    null|                null|   null|   null|    null|    null|    null|                null|5.397605346934028...|5.397605346934028...|    null|                null|   null|                null|                null|                null|    4.0|     2.0|     3.0|     3.0|     2.0|     2.0|    4.0|     2.0|     2.0|     2.0|     3.0|     1.0|     1.0|    null|    null|     9.0|\n",
      "|   1.0|   1.0|b'01202015'| b'01'|b'20'|b'2015'|  1200.0|2.015000018E9|2.015000018E9|     1.0|     1.0|    null|     1.0|     2.0|  null|     2.0|                 1.0|     1.0|    null|    null|  null|    null|    null|  null|    null|   null|    3.0|    88.0|    88.0|    null|     1.0|     1.0|    1.0|     1.0|    1.0|   1.0|     1.0|    1.0|    1.0|     2.0|     2.0|     2.0|    2.0|   null|     1.0|     2.0|     2.0|     2.0|     2.0|     2.0|     3.0|    null|1.0|    1.0|  5.0|     1.0|     2.0|    null|    1.0|     2.0|    7.0|    88.0|    7.0|     2.0|  178.0|  510.0|    null|     2.0|     2.0|  2.0|   2.0|     2.0|     2.0|     2.0|     1.0|     1.0|     2.0|    null|    3.0|  888.0|    null|    null|    null|    null|  null|   null|   null|   null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|   null|    null|    null|     1.0|     1.0|   null|    null|    null|    null|    null|   null|   null|   null|   null|    null|    null|    null|   null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|   null|    null|  null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|   null|    null|   null|   null|    null|    null|    null|  null|   null|   null|    null|   null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|   null|    null|    null|     b''|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|  null|   null|    null|  null|  null|   null|  null|   null|   null|  10.0|    1.0|                b''|         b''|   1.0|11011.0|28.781560195347467|     2.0|57.563120390694934|    null|   null|   null|    null|     1.0|0.6141246821308681|308.65617060779226|    1.0|     9.0|     2.0|     1.0|    2.0|   2.0|     1.0|     1.0|     3.0|     2.0|    1.0|    1.0|     2.0|  1.0|     1.0|     1.0|     1.0|    12.0|     2.0|  77.0|   6.0| 70.0|178.0| 8074.0|2554.0|     3.0|    2.0|     1.0|    3.0|    5.0|     1.0|     2.0|     2.0|5.397605346934028...|     1.0|5.397605346934028...|     1.0|                null|    null|                null|    null|    null|    null|                 2.0|                 4.0|5.397605346934028...|5.397605346934028...|    null|    null|    9.0|    9.0|   1.0|   1.0|                 1.0|                 1.0|     9.0|    null|                null|1764.9999999999998|             303.0|    null|                null|   null|   null|    null|    null|    null|                null|                null|                 9.0|    null|                null|   null|                null|                null|                null|    9.0|     9.0|     9.0|     9.0|     9.0|     9.0|    9.0|     9.0|     3.0|     3.0|     4.0|     9.0|     9.0|     9.0|     9.0|    null|\n",
      "|   1.0|   1.0|b'01272015'| b'01'|b'27'|b'2015'|  1100.0|2.015000019E9|2.015000019E9|     1.0|     1.0|    null|     1.0|     2.0|  null|     1.0|5.397605346934028...|     1.0|    null|    null|  null|    null|    null|  null|    null|   null|    3.0|    14.0|    88.0|    88.0|     1.0|     1.0|    2.0|     1.0|    1.0|   1.0|     1.0|    1.0|    1.0|     2.0|     1.0|     1.0|    2.0|   null|     1.0|     2.0|     2.0|     1.0|     2.0|     2.0|     3.0|    null|2.0|    3.0|  3.0|     1.0|     2.0|    null|    1.0|     2.0|    7.0|    88.0|   77.0|     2.0|  155.0|  505.0|    null|     1.0|     1.0|  2.0|   2.0|     2.0|     2.0|     2.0|     2.0|    null|    null|    null|    3.0|  888.0|    null|    null|    null|   101.0| 101.0|  101.0|  203.0|  203.0|   203.0|     1.0|    64.0|   103.0|    15.0|    88.0|    null|    null|   888.0|     2.0|     2.0|     3.0|     4.0|     1.0|     1.0| 82014.0|     1.0|     1.0|    2.0|    null|    null|     1.0|     1.0|   null|    null|    null|    null|    null|   null|   null|   null|   null|    null|    null|    null|   null|     2.0|    null|    null|    null|    null|    null|    null|    null|     7.0|    null|    null|    null|    null|    null|    null|    null|    null|    null|     2.0|   null|    null|  null|    null|    null|     1.0|   777.0|     2.0|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|   null|    null|   null|   null|    null|    null|    null|  null|   null|   null|    null|   null|    null|    null|    null|    null|     2.0|    null|     1.0|     2.0|     4.0|    null|    null|    null|    null|   null|    null|    null|     b''|     5.0|     5.0|    null|    null|     1.0|    35.0|    null|    null|    null|    null|    null|    null|    null|    null|    null|  null|   null|    null|  null|  null|   null|  null|   null|   null|  10.0|    1.0|                b''|         b''|   3.0|11011.0|28.781560195347467|     1.0|28.781560195347467|    null|   null|   null|    null|     1.0|0.6141246821308681|211.21029542946906|    1.0|     9.0|     2.0|     1.0|    2.0|   1.0|     1.0|     1.0|     3.0|     1.0|    1.0|    1.0|     2.0|  1.0|     1.0|     1.0|     1.0|    13.0|     2.0|  80.0|   6.0| 65.0|165.0| 7031.0|2579.0|     3.0|    2.0|     1.0|    1.0|    9.0|     4.0|     1.0|     2.0|5.397605346934028...|     1.0|5.397605346934028...|     1.0|               100.0|   100.0|               100.0|    43.0|    43.0|    43.0|5.397605346934028...|5.397605346934028...|                 1.0|                 1.0|   200.0|   229.0|    1.0|    1.0|   1.0|   1.0|5.397605346934028...|5.397605346934028...|     1.0|    35.0|5.397605346934028...|            1470.0|             252.0|     2.0|5.397605346934028...|   15.0|   null|  3000.0|    null|    45.0|5.397605346934028...|5.397605346934028...|5.397605346934028...|    90.0|5.397605346934028...|   90.0|                45.0|5.397605346934028...|                45.0|    3.0|     2.0|     2.0|     2.0|     2.0|     2.0|    4.0|     2.0|     2.0|     2.0|     3.0|     1.0|     1.0|     1.0|     1.0|     2.0|\n",
      "|   1.0|   1.0|b'01062015'| b'01'|b'06'|b'2015'|  1100.0| 2.01500002E9| 2.01500002E9|     1.0|     1.0|    null|     1.0|     2.0|  null|     2.0|                 1.0|     1.0|    null|    null|  null|    null|    null|  null|    null|   null|    2.0|    88.0|    88.0|    null|     1.0|     1.0|    2.0|     1.0|    3.0|  null|     1.0|    1.0|    2.0|     2.0|     2.0|     2.0|    2.0|   null|     2.0|     2.0|     2.0|     2.0|     1.0|     2.0|     3.0|    null|1.0|    1.0|  4.0|     1.0|     2.0|    null|    1.0|     2.0|    2.0|    88.0|    3.0|     2.0|  163.0|  509.0|    null|     2.0|     2.0|  2.0|   2.0|     2.0|     2.0|     2.0|     2.0|    null|    null|    null|    3.0|  888.0|    null|    null|    null|   555.0| 302.0|  303.0|  203.0|  201.0|   203.0|     2.0|    null|    null|    null|    null|    null|    null|   888.0|    null|    null|    null|    null|     1.0|     2.0|    null|    null|     2.0|    1.0|777777.0|     1.0|     1.0|     3.0|   null|    null|    null|    null|    null|   null|   null|   null|   null|    null|    null|    null|   null|     2.0|    null|    null|    null|    null|    null|    null|    null|     2.0|    null|    null|    null|    null|    null|    null|    null|    null|    null|     2.0|   null|    null|  null|    null|    null|     1.0|   201.0|     2.0|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|   null|    null|   null|   null|    null|    null|    null|  null|   null|   null|    null|   null|    null|    null|    null|    null|     2.0|    null|     1.0|     2.0|     3.0|    null|    null|    null|    null|   null|    null|    null|     b''|     5.0|     5.0|     1.0|    60.0|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|  null|   null|    null|  null|  null|   null|  null|   null|   null|  10.0|    1.0|                b''|         b''|   3.0|11011.0|28.781560195347467|     2.0|57.563120390694934|    null|   null|   null|    null|     1.0|0.6141246821308681| 362.1717261327721|    1.0|     1.0|     1.0|     1.0|    1.0|   2.0|     1.0|     1.0|     3.0|     2.0|    1.0|    1.0|     2.0|  1.0|     1.0|     1.0|     1.0|     8.0|     1.0|  58.0|   5.0| 69.0|175.0| 7394.0|2407.0|     2.0|    1.0|     1.0|    2.0|    2.0|     4.0|     1.0|     2.0|5.397605346934028...|     1.0|5.397605346934028...|     1.0|5.397605346934028...|     7.0|                10.0|    43.0|    14.0|    43.0|5.397605346934028...|5.397605346934028...|                 1.0|                 1.0|     7.0|   110.0|    2.0|    1.0|   1.0|   1.0|5.397605346934028...|5.397605346934028...|     2.0|    null|                null|            2810.0|             482.0|    null|                null|   null|   null|    null|    null|    null|                null|5.397605346934028...|5.397605346934028...|    null|                null|   null|                null|                null|                null|    4.0|     2.0|     3.0|     3.0|     2.0|     2.0|    4.0|     2.0|     3.0|     3.0|     4.0|     1.0|     1.0|    null|    null|     1.0|\n",
      "+------+------+-----------+------+-----+-------+--------+-------------+-------------+--------+--------+--------+--------+--------+------+--------+--------------------+--------+--------+--------+------+--------+--------+------+--------+-------+-------+--------+--------+--------+--------+--------+-------+--------+-------+------+--------+-------+-------+--------+--------+--------+-------+-------+--------+--------+--------+--------+--------+--------+--------+--------+---+-------+-----+--------+--------+--------+-------+--------+-------+--------+-------+--------+-------+-------+--------+--------+--------+-----+------+--------+--------+--------+--------+--------+--------+--------+-------+-------+--------+--------+--------+--------+------+-------+-------+-------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+-------+--------+--------+--------+--------+-------+--------+--------+--------+--------+-------+-------+-------+-------+--------+--------+--------+-------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+-------+--------+------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+-------+--------+-------+-------+--------+--------+--------+------+-------+-------+--------+-------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+-------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+------+-------+--------+------+------+-------+------+-------+-------+------+-------+-------------------+------------+------+-------+------------------+--------+------------------+--------+-------+-------+--------+--------+------------------+------------------+-------+--------+--------+--------+-------+------+--------+--------+--------+--------+-------+-------+--------+-----+--------+--------+--------+--------+--------+------+------+-----+-----+-------+------+--------+-------+--------+-------+-------+--------+--------+--------+--------------------+--------+--------------------+--------+--------------------+--------+--------------------+--------+--------+--------+--------------------+--------------------+--------------------+--------------------+--------+--------+-------+-------+------+------+--------------------+--------------------+--------+--------+--------------------+------------------+------------------+--------+--------------------+-------+-------+--------+--------+--------+--------------------+--------------------+--------------------+--------+--------------------+-------+--------------------+--------------------+--------------------+-------+--------+--------+--------+--------+--------+-------+--------+--------+--------+--------+--------+--------+--------+--------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"./data/diabetes_large/big_diabetes_data.csv\", \n",
    "                   inferSchema=True,\n",
    "                   header=True,\n",
    "                   sep=\",\")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62aaf9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_male = df.filter(df.SEX == 1)\n",
    "df_female = df.filter(df.SEX == 2)\n",
    "\n",
    "assert len(df_male.columns) == len(df.columns), \"Males have less number of columns\"\n",
    "assert len(df_female.columns) == len(df.columns), \"Females have less number of columns\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db53661d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-----------+------+-----+-------+--------+-------------+-------------+--------+--------+--------+--------+--------+------+--------+------+--------------------+--------+--------+------+--------+--------+------+--------+-------+-------+--------+--------+--------+--------+--------+-------+--------+-------+------+--------+-------+-------+--------+--------+--------+-------+-------+--------+--------+--------+--------+--------+--------+--------+--------+---+-------+-----+--------+--------+--------+-------+--------+-------+--------+-------+--------+-------+-------+--------+--------+--------+-----+------+--------+--------+--------+--------+--------+--------+--------+-------+-------+--------+--------+--------+--------+------+-------+-------+-------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+-------+--------+--------+--------+--------+-------+--------+--------+--------+--------+-------+-------+-------+-------+--------+--------+--------+-------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+-------+--------+------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+-------+--------+-------+-------+--------+--------+--------+------+-------+-------+--------+-------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+-------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+------+-------+--------+------+------+-------+------+-------+-------+------+-------+--------+------------+------+-------+------------------+--------+------------------+--------+-------+-------+--------+--------+------------------+------------------+-------+--------+--------+--------+-------+------+--------+--------+--------+--------+-------+-------+--------+-----+--------+--------+--------+--------+--------+------+------+-----+-----+------+------+--------+-------+--------+-------+-------+--------+--------+--------+--------------------+--------+--------------------+--------+--------------------+--------+--------+--------------------+--------+--------+--------------------+--------------------+--------------------+--------------------+--------+--------+-------+-------+------+------+--------------------+--------------------+--------+--------+--------------------+------------------+-----+--------+--------------------+-------+-------+--------+--------+--------+--------------------+--------------------+--------------------+--------+--------------------+-------+--------------------+--------------------+--------------------+-------+--------+--------+--------+--------+--------+-------+--------+--------+--------+--------+--------+--------+--------+--------+--------+\n",
      "|_STATE|FMONTH|      IDATE|IMONTH| IDAY|  IYEAR|DISPCODE|        SEQNO|         _PSU|CTELENUM|PVTRESD1|COLGHOUS|STATERES|CELLFON3|LADULT|NUMADULT|NUMMEN|            NUMWOMEN|CTELNUM1|CELLFON2|CADULT|PVTRESD2|CCLGHOUS|CSTATE|LANDLINE|HHADULT|GENHLTH|PHYSHLTH|MENTHLTH|POORHLTH|HLTHPLN1|PERSDOC2|MEDCOST|CHECKUP1|BPHIGH4|BPMEDS|BLOODCHO|CHOLCHK|TOLDHI2|CVDINFR4|CVDCRHD4|CVDSTRK3|ASTHMA3|ASTHNOW|CHCSCNCR|CHCOCNCR|CHCCOPD1|HAVARTH3|ADDEPEV2|CHCKIDNY|DIABETE3|DIABAGE2|SEX|MARITAL|EDUCA|RENTHOM1|NUMHHOL2|NUMPHON2|CPDEMO1|VETERAN3|EMPLOY1|CHILDREN|INCOME2|INTERNET|WEIGHT2|HEIGHT3|PREGNANT|QLACTLM2|USEEQUIP|BLIND|DECIDE|DIFFWALK|DIFFDRES|DIFFALON|SMOKE100|SMOKDAY2|STOPSMK2|LASTSMK2|USENOW3|ALCDAY5|AVEDRNK2|DRNK3GE5|MAXDRNKS|FRUITJU1|FRUIT1|FVBEANS|FVGREEN|FVORANG|VEGETAB1|EXERANY2|EXRACT11|EXEROFT1|EXERHMM1|EXRACT21|EXEROFT2|EXERHMM2|STRENGTH|LMTJOIN3|ARTHDIS2|ARTHSOCL|JOINPAIN|SEATBELT|FLUSHOT6|FLSHTMY2|IMFVPLAC|PNEUVAC3|HIVTST6|HIVTSTD3|WHRTST10|PDIABTST|PREDIAB1|INSULIN|BLDSUGAR|FEETCHK2|DOCTDIAB|CHKHEMO3|FEETCHK|EYEEXAM|DIABEYE|DIABEDU|PAINACT2|QLMENTL2|QLSTRES2|QLHLTH2|CAREGIV1|CRGVREL1|CRGVLNG1|CRGVHRS1|CRGVPRB1|CRGVPERS|CRGVHOUS|CRGVMST2|CRGVEXPT|VIDFCLT2|VIREDIF3|VIPRFVS2|VINOCRE2|VIEYEXM2|VIINSUR2|VICTRCT4|VIGLUMA2|VIMACDG2|CIMEMLOS|CDHOUSE|CDASSIST|CDHELP|CDSOCIAL|CDDISCUS|WTCHSALT|LONGWTCH|DRADVISE|ASTHMAGE|ASATTACK|ASERVIST|ASDRVIST|ASRCHKUP|ASACTLIM|ASYMPTOM|ASNOSLEP|ASTHMED3|ASINHALR|HAREHAB1|STREHAB1|CVDASPRN|ASPUNSAF|RLIVPAIN|RDUCHART|RDUCSTRK|ARTTODAY|ARTHWGT|ARTHEXER|ARTHEDU|TETANUS|HPVADVC2|HPVADSHT|SHINGLE2|HADMAM|HOWLONG|HADPAP2|LASTPAP2|HPVTEST|HPLSTTST|HADHYST2|PROFEXAM|LENGEXAM|BLDSTOOL|LSTBLDS3|HADSIGM3|HADSGCO1|LASTSIG3|PCPSAAD2|PCPSADI1|PCPSARE1|PSATEST1|PSATIME|PCPSARS1|PCPSADE1|PCDMDECN|SCNTMNY1|SCNTMEL1|SCNTPAID|SCNTWRK1|SCNTLPAD|SCNTLWK1|SXORIENT|TRNSGNDR|RCSGENDR|RCSRLTN2|CASTHDX2|CASTHNO2|EMTSUPRT|LSATISFY|ADPLEASR|ADDOWN|ADSLEEP|ADENERGY|ADEAT1|ADFAIL|ADTHINK|ADMOVE|MISTMNT|ADANXEV|QSTVER|QSTLANG|EXACTOT1|    EXACTOT2|MSCODE| _STSTR|            _STRWT|_RAWRAKE|          _WT2RAKE|_CHISPNC|_CRACE1|_CPRACE|_CLLCPWT|_DUALUSE|          _DUALCOR|           _LLCPWT|_RFHLTH|_HCVU651|_RFHYPE5|_CHOLCHK|_RFCHOL|_MICHD|_LTASTH1|_CASTHM1|_ASTHMS1|_DRDXAR1|_PRACE1|_MRACE1|_HISPANC|_RACE|_RACEG21|_RACEGR3|_RACE_G1|_AGEG5YR|_AGE65YR|_AGE80|_AGE_G|HTIN4| HTM4| WTKG3| _BMI5|_BMI5CAT|_RFBMI5|_CHLDCNT|_EDUCAG|_INCOMG|_SMOKER3|_RFSMOK3|DRNKANY5|            DROCDY3_|_RFBING5|            _DRNKWEK|_RFDRHV5|            FTJUDA1_|FRUTDA1_|BEANDAY_|            GRENDAY_|ORNGDAY_|VEGEDA1_|            _MISFRTN|            _MISVEGN|            _FRTRESP|            _VEGRESP|_FRUTSUM|_VEGESUM|_FRTLT1|_VEGLT1|_FRT16|_VEG23|            _FRUITEX|            _VEGETEX|_TOTINDA|METVL11_|            METVL21_|           MAXVO2_|FC60_|ACTIN11_|            ACTIN21_|PADUR1_|PADUR2_|PAFREQ1_|PAFREQ2_|_MINAC11|            _MINAC21|            STRFREQ_|            PAMISS1_|PAMIN11_|            PAMIN21_|PA1MIN_|            PAVIG11_|            PAVIG21_|            PA1VIGM_|_PACAT1|_PAINDX1|_PA150R2|_PA300R2|_PA30021|_PASTRNG|_PAREC1|_PASTAE1|_LMTACT1|_LMTWRK1|_LMTSCL1|_RFSEAT2|_RFSEAT3|_FLSHOT6|_PNEUMO2|_AIDTST3|\n",
      "+------+------+-----------+------+-----+-------+--------+-------------+-------------+--------+--------+--------+--------+--------+------+--------+------+--------------------+--------+--------+------+--------+--------+------+--------+-------+-------+--------+--------+--------+--------+--------+-------+--------+-------+------+--------+-------+-------+--------+--------+--------+-------+-------+--------+--------+--------+--------+--------+--------+--------+--------+---+-------+-----+--------+--------+--------+-------+--------+-------+--------+-------+--------+-------+-------+--------+--------+--------+-----+------+--------+--------+--------+--------+--------+--------+--------+-------+-------+--------+--------+--------+--------+------+-------+-------+-------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+-------+--------+--------+--------+--------+-------+--------+--------+--------+--------+-------+-------+-------+-------+--------+--------+--------+-------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+-------+--------+------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+-------+--------+-------+-------+--------+--------+--------+------+-------+-------+--------+-------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+-------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+------+-------+--------+------+------+-------+------+-------+-------+------+-------+--------+------------+------+-------+------------------+--------+------------------+--------+-------+-------+--------+--------+------------------+------------------+-------+--------+--------+--------+-------+------+--------+--------+--------+--------+-------+-------+--------+-----+--------+--------+--------+--------+--------+------+------+-----+-----+------+------+--------+-------+--------+-------+-------+--------+--------+--------+--------------------+--------+--------------------+--------+--------------------+--------+--------+--------------------+--------+--------+--------------------+--------------------+--------------------+--------------------+--------+--------+-------+-------+------+------+--------------------+--------------------+--------+--------+--------------------+------------------+-----+--------+--------------------+-------+-------+--------+--------+--------+--------------------+--------------------+--------------------+--------+--------------------+-------+--------------------+--------------------+--------------------+-------+--------+--------+--------+--------+--------+-------+--------+--------+--------+--------+--------+--------+--------+--------+--------+\n",
      "|   1.0|   1.0|b'01142015'| b'01'|b'14'|b'2015'|  1100.0|2.015000008E9|2.015000008E9|     1.0|     1.0|    null|     1.0|     2.0|  null|     2.0|   1.0|                 1.0|    null|    null|  null|    null|    null|  null|    null|   null|    5.0|     8.0|    88.0|     8.0|     1.0|     1.0|    1.0|     1.0|    1.0|   1.0|     1.0|    1.0|    1.0|     7.0|     2.0|     2.0|    2.0|   null|     2.0|     2.0|     2.0|     1.0|     2.0|     7.0|     3.0|    null|1.0|    1.0|  3.0|     1.0|     2.0|    null|    1.0|     1.0|    3.0|    88.0|    3.0|     2.0|  179.0|  501.0|    null|     1.0|     2.0|  2.0|   2.0|     1.0|     2.0|     2.0|     1.0|     3.0|    null|     7.0|    3.0|  888.0|    null|    null|    null|   102.0| 101.0|  202.0|  101.0|  303.0|   202.0|     1.0|    64.0|   106.0|    12.0|    98.0|   107.0|     5.0|   888.0|     1.0|     1.0|     1.0|    77.0|     1.0|     1.0|122014.0|     1.0|     1.0|    1.0|777777.0|     4.0|     1.0|     3.0|   null|    null|    null|    null|    null|   null|   null|   null|   null|    null|    null|    null|   null|     2.0|    null|    null|    null|    null|    null|    null|    null|     2.0|    null|    null|    null|    null|    null|    null|    null|    null|    null|     2.0|   null|    null|  null|    null|    null|     1.0|   777.0|     2.0|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|   null|    null|   null|   null|    null|    null|    null|  null|   null|   null|    null|   null|    null|    null|    null|    null|     2.0|    null|     1.0|     2.0|     4.0|    null|    null|    null|    null|   null|    null|    null|     b''|     4.0|     2.0|    null|    null|     1.0|    45.0|    null|    null|    null|    null|    null|    null|    null|    null|    null|  null|   null|    null|  null|  null|   null|  null|   null|   null|  10.0|    1.0|     b''|b'Treadmill'|   5.0|11011.0|28.781560195347467|     2.0|57.563120390694934|    null|   null|   null|    null|     1.0|0.6141246821308681| 545.7820952341142|    2.0|     9.0|     2.0|     1.0|    2.0|  null|     1.0|     1.0|     3.0|     1.0|    1.0|    1.0|     2.0|  1.0|     1.0|     1.0|     1.0|    13.0|     2.0|  80.0|   6.0| 61.0|155.0|8119.0|3382.0|     4.0|    2.0|     1.0|    1.0|    2.0|     3.0|     1.0|     2.0|5.397605346934028...|     1.0|5.397605346934028...|     1.0|               200.0|   100.0|    29.0|               100.0|    10.0|    29.0|5.397605346934028...|5.397605346934028...|                 1.0|                 1.0|   300.0|   168.0|    1.0|    1.0|   1.0|   1.0|5.397605346934028...|5.397605346934028...|     1.0|    35.0|                45.0|            1545.0|265.0|     2.0|                 2.0|   12.0|    5.0|  6000.0|  7000.0|    72.0|5.397605346934028...|5.397605346934028...|5.397605346934028...|   144.0|5.397605346934028...|  144.0|                72.0|5.397605346934028...|                72.0|    3.0|     2.0|     2.0|     2.0|     2.0|     2.0|    4.0|     2.0|     1.0|     1.0|     1.0|     1.0|     1.0|     1.0|     1.0|     1.0|\n",
      "|   1.0|   1.0|b'01302015'| b'01'|b'30'|b'2015'|  1100.0| 2.01500001E9| 2.01500001E9|     1.0|     1.0|    null|     1.0|     2.0|  null|     2.0|   1.0|                 1.0|    null|    null|  null|    null|    null|  null|    null|   null|    2.0|     2.0|    88.0|     2.0|     1.0|     1.0|    2.0|     1.0|    1.0|   1.0|     1.0|    1.0|    1.0|     2.0|     2.0|     2.0|    2.0|   null|     2.0|     2.0|     2.0|     2.0|     2.0|     2.0|     3.0|    null|1.0|    1.0|  6.0|     1.0|     2.0|    null|    1.0|     2.0|    7.0|    88.0|    8.0|     1.0|  161.0|  507.0|    null|     2.0|     2.0|  2.0|   2.0|     2.0|     2.0|     2.0|     1.0|     3.0|    null|     7.0|    3.0|  888.0|    null|    null|    null|   201.0| 101.0|  204.0|  205.0|  206.0|   101.0|     1.0|    64.0|   106.0|    50.0|    88.0|    null|    null|   888.0|    null|    null|    null|    null|     2.0|     2.0|    null|    null|     2.0|    2.0|    null|    null|     1.0|     3.0|   null|    null|    null|    null|    null|   null|   null|   null|   null|    null|    null|    null|   null|     2.0|    null|    null|    null|    null|    null|    null|    null|     2.0|    null|    null|    null|    null|    null|    null|    null|    null|    null|     2.0|   null|    null|  null|    null|    null|     1.0|   415.0|     1.0|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|   null|    null|   null|   null|    null|    null|    null|  null|   null|   null|    null|   null|    null|    null|    null|    null|     2.0|    null|     1.0|     2.0|     2.0|    null|    null|    null|    null|   null|    null|    null|     b''|     5.0|     5.0|    null|    null|     1.0|    60.0|    null|    null|    null|    null|    null|    null|    null|    null|    null|  null|   null|    null|  null|  null|   null|  null|   null|   null|  10.0|    1.0|     b''|         b''|   1.0|11011.0|28.781560195347467|     2.0|57.563120390694934|    null|   null|   null|    null|     1.0|0.6141246821308681|215.47286326207018|    1.0|     9.0|     2.0|     1.0|    2.0|   2.0|     1.0|     1.0|     3.0|     2.0|    1.0|    1.0|     2.0|  1.0|     1.0|     1.0|     1.0|    10.0|     2.0|  68.0|   6.0| 67.0|170.0|7303.0|2522.0|     3.0|    2.0|     1.0|    4.0|    5.0|     3.0|     1.0|     2.0|5.397605346934028...|     1.0|5.397605346934028...|     1.0|                14.0|   100.0|    57.0|                71.0|    86.0|   100.0|5.397605346934028...|5.397605346934028...|                 1.0|                 1.0|   114.0|   314.0|    1.0|    1.0|   1.0|   1.0|5.397605346934028...|5.397605346934028...|     1.0|    35.0|5.397605346934028...|            2260.0|387.0|     1.0|5.397605346934028...|   50.0|   null|  6000.0|    null|   300.0|5.397605346934028...|5.397605346934028...|5.397605346934028...|   300.0|5.397605346934028...|  300.0|5.397605346934028...|5.397605346934028...|5.397605346934028...|    2.0|     1.0|     1.0|     2.0|     2.0|     2.0|    2.0|     2.0|     3.0|     3.0|     4.0|     1.0|     2.0|     2.0|     2.0|     2.0|\n",
      "|   1.0|   1.0|b'01202015'| b'01'|b'20'|b'2015'|  1200.0|2.015000018E9|2.015000018E9|     1.0|     1.0|    null|     1.0|     2.0|  null|     2.0|   1.0|                 1.0|    null|    null|  null|    null|    null|  null|    null|   null|    3.0|    88.0|    88.0|    null|     1.0|     1.0|    1.0|     1.0|    1.0|   1.0|     1.0|    1.0|    1.0|     2.0|     2.0|     2.0|    2.0|   null|     1.0|     2.0|     2.0|     2.0|     2.0|     2.0|     3.0|    null|1.0|    1.0|  5.0|     1.0|     2.0|    null|    1.0|     2.0|    7.0|    88.0|    7.0|     2.0|  178.0|  510.0|    null|     2.0|     2.0|  2.0|   2.0|     2.0|     2.0|     2.0|     1.0|     1.0|     2.0|    null|    3.0|  888.0|    null|    null|    null|    null|  null|   null|   null|   null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|   null|    null|    null|     1.0|     1.0|   null|    null|    null|    null|    null|   null|   null|   null|   null|    null|    null|    null|   null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|   null|    null|  null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|   null|    null|   null|   null|    null|    null|    null|  null|   null|   null|    null|   null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|   null|    null|    null|     b''|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|  null|   null|    null|  null|  null|   null|  null|   null|   null|  10.0|    1.0|     b''|         b''|   1.0|11011.0|28.781560195347467|     2.0|57.563120390694934|    null|   null|   null|    null|     1.0|0.6141246821308681|308.65617060779226|    1.0|     9.0|     2.0|     1.0|    2.0|   2.0|     1.0|     1.0|     3.0|     2.0|    1.0|    1.0|     2.0|  1.0|     1.0|     1.0|     1.0|    12.0|     2.0|  77.0|   6.0| 70.0|178.0|8074.0|2554.0|     3.0|    2.0|     1.0|    3.0|    5.0|     1.0|     2.0|     2.0|5.397605346934028...|     1.0|5.397605346934028...|     1.0|                null|    null|    null|                null|    null|    null|                 2.0|                 4.0|5.397605346934028...|5.397605346934028...|    null|    null|    9.0|    9.0|   1.0|   1.0|                 1.0|                 1.0|     9.0|    null|                null|1764.9999999999998|303.0|    null|                null|   null|   null|    null|    null|    null|                null|                null|                 9.0|    null|                null|   null|                null|                null|                null|    9.0|     9.0|     9.0|     9.0|     9.0|     9.0|    9.0|     9.0|     3.0|     3.0|     4.0|     9.0|     9.0|     9.0|     9.0|    null|\n",
      "|   1.0|   1.0|b'01062015'| b'01'|b'06'|b'2015'|  1100.0| 2.01500002E9| 2.01500002E9|     1.0|     1.0|    null|     1.0|     2.0|  null|     2.0|   1.0|                 1.0|    null|    null|  null|    null|    null|  null|    null|   null|    2.0|    88.0|    88.0|    null|     1.0|     1.0|    2.0|     1.0|    3.0|  null|     1.0|    1.0|    2.0|     2.0|     2.0|     2.0|    2.0|   null|     2.0|     2.0|     2.0|     2.0|     1.0|     2.0|     3.0|    null|1.0|    1.0|  4.0|     1.0|     2.0|    null|    1.0|     2.0|    2.0|    88.0|    3.0|     2.0|  163.0|  509.0|    null|     2.0|     2.0|  2.0|   2.0|     2.0|     2.0|     2.0|     2.0|    null|    null|    null|    3.0|  888.0|    null|    null|    null|   555.0| 302.0|  303.0|  203.0|  201.0|   203.0|     2.0|    null|    null|    null|    null|    null|    null|   888.0|    null|    null|    null|    null|     1.0|     2.0|    null|    null|     2.0|    1.0|777777.0|     1.0|     1.0|     3.0|   null|    null|    null|    null|    null|   null|   null|   null|   null|    null|    null|    null|   null|     2.0|    null|    null|    null|    null|    null|    null|    null|     2.0|    null|    null|    null|    null|    null|    null|    null|    null|    null|     2.0|   null|    null|  null|    null|    null|     1.0|   201.0|     2.0|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|   null|    null|   null|   null|    null|    null|    null|  null|   null|   null|    null|   null|    null|    null|    null|    null|     2.0|    null|     1.0|     2.0|     3.0|    null|    null|    null|    null|   null|    null|    null|     b''|     5.0|     5.0|     1.0|    60.0|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|  null|   null|    null|  null|  null|   null|  null|   null|   null|  10.0|    1.0|     b''|         b''|   3.0|11011.0|28.781560195347467|     2.0|57.563120390694934|    null|   null|   null|    null|     1.0|0.6141246821308681| 362.1717261327721|    1.0|     1.0|     1.0|     1.0|    1.0|   2.0|     1.0|     1.0|     3.0|     2.0|    1.0|    1.0|     2.0|  1.0|     1.0|     1.0|     1.0|     8.0|     1.0|  58.0|   5.0| 69.0|175.0|7394.0|2407.0|     2.0|    1.0|     1.0|    2.0|    2.0|     4.0|     1.0|     2.0|5.397605346934028...|     1.0|5.397605346934028...|     1.0|5.397605346934028...|     7.0|    10.0|                43.0|    14.0|    43.0|5.397605346934028...|5.397605346934028...|                 1.0|                 1.0|     7.0|   110.0|    2.0|    1.0|   1.0|   1.0|5.397605346934028...|5.397605346934028...|     2.0|    null|                null|            2810.0|482.0|    null|                null|   null|   null|    null|    null|    null|                null|5.397605346934028...|5.397605346934028...|    null|                null|   null|                null|                null|                null|    4.0|     2.0|     3.0|     3.0|     2.0|     2.0|    4.0|     2.0|     3.0|     3.0|     4.0|     1.0|     1.0|    null|    null|     1.0|\n",
      "|   1.0|   1.0|b'01142015'| b'01'|b'14'|b'2015'|  1100.0|2.015000021E9|2.015000021E9|     1.0|     1.0|    null|     1.0|     2.0|  null|     1.0|   1.0|5.397605346934028...|    null|    null|  null|    null|    null|  null|    null|   null|    3.0|    88.0|    88.0|    null|     1.0|     1.0|    7.0|     1.0|    1.0|   1.0|     1.0|    1.0|    1.0|     1.0|     2.0|     2.0|    2.0|   null|     2.0|     2.0|     2.0|     2.0|     2.0|     2.0|     3.0|    null|1.0|    1.0|  3.0|     1.0|     2.0|    null|    1.0|     1.0|    7.0|    88.0|    4.0|     2.0|  170.0|  507.0|    null|     1.0|     2.0|  2.0|   2.0|     2.0|     2.0|     2.0|     1.0|     3.0|    null|     7.0|    3.0|  888.0|    null|    null|    null|   555.0| 101.0|  101.0|  555.0|  201.0|   101.0|     2.0|    null|    null|    null|    null|    null|    null|   888.0|    null|    null|    null|    null|     1.0|     2.0|    null|    null|     1.0|    2.0|    null|    null|     1.0|     3.0|   null|    null|    null|    null|    null|   null|   null|   null|   null|    null|    null|    null|   null|     2.0|    null|    null|    null|    null|    null|    null|    null|     2.0|    null|    null|    null|    null|    null|    null|    null|    null|    null|     2.0|   null|    null|  null|    null|    null|     2.0|    null|     1.0|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|    null|   null|    null|   null|   null|    null|    null|    null|  null|   null|   null|    null|   null|    null|    null|    null|    null|     1.0|     3.0|     1.0|     2.0|     3.0|    null|    null|    null|    null|   null|    null|    null|     b''|     5.0|     5.0|    null|    null|     3.0|    40.0|    null|    null|    null|    null|    null|    null|    null|    null|    null|  null|   null|    null|  null|  null|   null|  null|   null|   null|  10.0|    1.0|     b''|         b''|   5.0|11011.0|28.781560195347467|     1.0|28.781560195347467|    null|   null|   null|    null|     1.0|0.6141246821308681|197.90361983767596|    1.0|     9.0|     2.0|     1.0|    2.0|   1.0|     1.0|     1.0|     3.0|     2.0|    1.0|    7.0|     2.0|  7.0|     2.0|     4.0|     5.0|    13.0|     2.0|  80.0|   6.0| 67.0|170.0|7711.0|2663.0|     3.0|    2.0|     1.0|    1.0|    2.0|     3.0|     1.0|     2.0|5.397605346934028...|     1.0|5.397605346934028...|     1.0|5.397605346934028...|   100.0|   100.0|5.397605346934028...|    14.0|   100.0|5.397605346934028...|5.397605346934028...|                 1.0|                 1.0|   100.0|   214.0|    1.0|    1.0|   1.0|   1.0|5.397605346934028...|5.397605346934028...|     2.0|    null|                null|            1545.0|265.0|    null|                null|   null|   null|    null|    null|    null|                null|5.397605346934028...|5.397605346934028...|    null|                null|   null|                null|                null|                null|    4.0|     2.0|     3.0|     3.0|     2.0|     2.0|    4.0|     2.0|     3.0|     3.0|     4.0|     1.0|     1.0|     2.0|     1.0|     2.0|\n",
      "+------+------+-----------+------+-----+-------+--------+-------------+-------------+--------+--------+--------+--------+--------+------+--------+------+--------------------+--------+--------+------+--------+--------+------+--------+-------+-------+--------+--------+--------+--------+--------+-------+--------+-------+------+--------+-------+-------+--------+--------+--------+-------+-------+--------+--------+--------+--------+--------+--------+--------+--------+---+-------+-----+--------+--------+--------+-------+--------+-------+--------+-------+--------+-------+-------+--------+--------+--------+-----+------+--------+--------+--------+--------+--------+--------+--------+-------+-------+--------+--------+--------+--------+------+-------+-------+-------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+-------+--------+--------+--------+--------+-------+--------+--------+--------+--------+-------+-------+-------+-------+--------+--------+--------+-------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+-------+--------+------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+-------+--------+-------+-------+--------+--------+--------+------+-------+-------+--------+-------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+-------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+------+-------+--------+------+------+-------+------+-------+-------+------+-------+--------+------------+------+-------+------------------+--------+------------------+--------+-------+-------+--------+--------+------------------+------------------+-------+--------+--------+--------+-------+------+--------+--------+--------+--------+-------+-------+--------+-----+--------+--------+--------+--------+--------+------+------+-----+-----+------+------+--------+-------+--------+-------+-------+--------+--------+--------+--------------------+--------+--------------------+--------+--------------------+--------+--------+--------------------+--------+--------+--------------------+--------------------+--------------------+--------------------+--------+--------+-------+-------+------+------+--------------------+--------------------+--------+--------+--------------------+------------------+-----+--------+--------------------+-------+-------+--------+--------+--------+--------------------+--------------------+--------------------+--------+--------------------+-------+--------------------+--------------------+--------------------+-------+--------+--------+--------+--------+--------+-------+--------+--------+--------+--------+--------+--------+--------+--------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_male.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e129ec1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n"
     ]
    }
   ],
   "source": [
    "df_male.toPandas().to_csv(\"./data/diabetes_large/male_data.csv\", index=False)\n",
    "df_female.toPandas().to_csv(\"./data/diabetes_large/female_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bd5409",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
