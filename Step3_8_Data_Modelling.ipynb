{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eae0cc21-0345-4717-a878-0219248debda",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Dict, Any, Optional, Callable\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "import pyspark.sql.functions as F\n",
    "import os\n",
    "import random\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import IntegerType, FloatType, DoubleType\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.classification import DecisionTreeClassifier, RandomForestClassifier, GBTClassifier\n",
    "from pyspark.ml.tuning import CrossValidator, CrossValidatorModel, ParamGridBuilder\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, OneHotEncoder\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bdd0e97-a021-4bae-a49e-5808059995b1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/12 20:40:26 WARN Utils: Your hostname, darkstar resolves to a loopback address: 127.0.1.1; using 192.168.1.208 instead (on interface wlp5s0)\n",
      "23/10/12 20:40:26 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/10/12 20:40:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"diabetes_indicators\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc891114-bce6-421d-afa2-5a1507cf27c3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+--------+-------+------+--------+--------+------+--------+-------+-------+--------+-------+-------+--------+--------+---+--------+-------+-----+-------+\n",
      "|DIABETE3|CHCKIDNY|_RFHYPE5|TOLDHI2| _BMI5|SMOKE100|CVDSTRK3|_MICHD|_TOTINDA|_FRTLT1|_VEGLT1|_RFDRHV5|MEDCOST|GENHLTH|PHYSHLTH|MENTHLTH|SEX|_AGEG5YR|_MRACE1|EDUCA|INCOME2|\n",
      "+--------+--------+--------+-------+------+--------+--------+------+--------+-------+-------+--------+-------+-------+--------+--------+---+--------+-------+-----+-------+\n",
      "|       0|       0|       1|      1|2522.0|       1|       0|     0|       1|      1|      1|       0|      0|    2.0|     2.0|    88.0|  0|    10.0|    1.0|  6.0|    8.0|\n",
      "|       0|       0|       0|      0|2407.0|       0|       0|     0|       0|      0|      1|       0|      0|    2.0|    88.0|    88.0|  0|     8.0|    1.0|  4.0|    3.0|\n",
      "|       2|       0|       0|      0|2468.0|       1|       0|     0|       1|      1|      1|       0|      0|    3.0|    88.0|    88.0|  0|    13.0|    1.0|  6.0|    8.0|\n",
      "|       2|       0|       0|      0|2317.0|       1|       0|     0|       1|      0|      0|       0|      0|    2.0|    88.0|    88.0|  0|     7.0|    1.0|  5.0|    6.0|\n",
      "|       0|       0|       0|      1|2800.0|       0|       0|     0|       0|      0|      0|       1|      0|    2.0|    88.0|    10.0|  0|     4.0|    1.0|  6.0|    8.0|\n",
      "+--------+--------+--------+-------+------+--------+--------+------+--------+-------+-------+--------+-------+-------+--------+--------+---+--------+-------+-----+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"./data/diabetes/final_diabetes_dataset.csv\", header=True, inferSchema=True, sep=\",\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d54dad-6b81-451a-9d46-afd8235167ea",
   "metadata": {},
   "source": [
    "# Feature Scaling and Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d22c0161-9345-4167-b412-9315776704b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def transform_data(df: pyspark.sql.DataFrame, map_dict: dict, colName: str) -> pyspark.sql.DataFrame:\n",
    "    \"\"\" Function to transform predictor variable based on map_dict \"\"\"\n",
    "    map_col = F.create_map([F.lit(x) for i in map_dict.items() for x in i])\n",
    "    new_df = df.withColumn(colName, map_col[F.col(colName)])\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "582baa3e-eb30-4665-9fab-28389154d310",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.withColumn(\"PHYSHLTH\", F.when(df[\"PHYSHLTH\"] == 88, 0).otherwise(df[\"PHYSHLTH\"]))\n",
    "df = df.withColumn(\"MENTHLTH\", F.when(df[\"MENTHLTH\"] == 88, 0).otherwise(df[\"MENTHLTH\"]))\n",
    "\n",
    "df = transform_data(df, {v:idx for idx, v in enumerate(range(1, 14))}, \"_AGEG5YR\")\n",
    "df = transform_data(df, {v:idx for idx, v in enumerate(range(1, 8))}, \"_MRACE1\")\n",
    "df = transform_data(df, {v:idx for idx, v in enumerate(range(1, 7))}, \"EDUCA\")\n",
    "df = transform_data(df, {v:idx for idx, v in enumerate(range(1, 9))}, \"INCOME2\")\n",
    "df = transform_data(df, {v:idx for idx, v in enumerate(range(1, 6))}, \"GENHLTH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1d5e1d9-87fd-437e-8ba4-57be9dd461b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/12 20:40:36 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|     numericfeatures|            features|\n",
      "+--------------------+--------------------+\n",
      "|    [2522.0,2.0,0.0]|(20,[0,1,4,5,6,9,...|\n",
      "|    [2407.0,0.0,0.0]|(20,[0,11,14,16,1...|\n",
      "|    [2468.0,0.0,0.0]|(20,[0,6,9,10,11,...|\n",
      "|    [2317.0,0.0,0.0]|(20,[0,6,9,14,16,...|\n",
      "|   [2800.0,0.0,10.0]|(20,[0,2,5,12,14,...|\n",
      "|[2839.34065274683...|(20,[0,4,6,9,10,1...|\n",
      "|    [2824.0,7.0,0.0]|(20,[0,1,6,11,14,...|\n",
      "|    [3723.0,0.0,0.0]|[3723.0,0.0,0.0,1...|\n",
      "|    [2789.0,0.0,0.0]|(20,[0,4,5,6,8,11...|\n",
      "|    [3119.0,0.0,0.0]|(20,[0,5,6,9,10,1...|\n",
      "|    [3325.0,0.0,0.0]|(20,[0,4,6,9,10,1...|\n",
      "|    [3085.0,0.0,0.0]|(20,[0,4,9,14,16,...|\n",
      "|    [2441.0,3.0,5.0]|[2441.0,3.0,5.0,0...|\n",
      "|    [2577.0,1.0,0.0]|(20,[0,1,6,9,10,1...|\n",
      "|   [2443.0,30.0,0.0]|(20,[0,1,4,5,6,10...|\n",
      "|    [2507.0,0.0,1.0]|(20,[0,2,4,5,6,8,...|\n",
      "|   [3673.0,2.0,30.0]|(20,[0,1,2,4,5,9,...|\n",
      "|   [2859.0,10.0,0.0]|(20,[0,1,9,11,16,...|\n",
      "|    [3338.0,0.0,1.0]|(20,[0,2,4,5,6,8,...|\n",
      "|   [3759.0,5.0,10.0]|[3759.0,5.0,10.0,...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_temp = VectorAssembler(inputCols=[\"_BMI5\", \"PHYSHLTH\", \"MENTHLTH\"], outputCol=\"numericfeatures\")\n",
    "_temp2 = VectorAssembler(inputCols=[x for x in df.columns if x not in [\"_BMI5\", \"PHYSHLTH\", \"MENTHLTH\", \"DIABETE3\"]], outputCol=\"catfeatures\")\n",
    "_temp3 = VectorAssembler(inputCols=[\"numericfeatures\", \"catfeatures\"], outputCol=\"features\")\n",
    "_temp3.transform(_temp2.transform(_temp.transform(df))).select(\"numericfeatures\",\"features\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a196ff8-2f46-46d1-9dcf-45d27e226c28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def scale_data(df: pyspark.sql.DataFrame, feats: List) -> pyspark.sql.DataFrame: \n",
    "    \"\"\" Function to scale the data \"\"\"\n",
    "    def scale_feat(df: pyspark.sql.DataFrame, feat: str) -> pyspark.sql.DataFrame:\n",
    "        \"\"\" Function to scale numeric columns of dataframe \"\"\"\n",
    "        unlist = F.udf(lambda x: round(float(list(x)[0]), 3), DoubleType())\n",
    "\n",
    "        assembler = VectorAssembler(inputCols=[feat], outputCol=feat+\"_vec\")\n",
    "        scaler = StandardScaler(inputCol=feat+\"_vec\", outputCol=feat+\"_scaled\")\n",
    "        pipeline = Pipeline(stages=[\n",
    "            assembler,\n",
    "            scaler\n",
    "        ])\n",
    "\n",
    "        if not os.path.exists(f\"./scalers/{feat}_pipeline\"):\n",
    "            pipeline_model = pipeline.fit(df)\n",
    "            pipeline_model.save(f\"./scalers/{feat}_pipeline\")\n",
    "\n",
    "        else:\n",
    "            print(f\"Loading pipeline from : ./scalers/{feat}_pipeline\")\n",
    "            pipeline_model = PipelineModel.load(f\"./scalers/{feat}_pipeline\")\n",
    "\n",
    "        df = pipeline_model.transform(df).withColumn(feat+\"_scaled\", unlist(feat+\"_scaled\")).drop(feat+\"_vec\")\n",
    "\n",
    "        return df, feat+\"_scaled\"\n",
    "    \n",
    "    new_feats = list()\n",
    "    for feat in feats:\n",
    "        df, _name = scale_feat(df, feat)\n",
    "        new_feats.append(_name)\n",
    "        \n",
    "    return df, new_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9f65ff3-b0d2-4592-b01c-1c8ea26205b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(\"./scalers/\"):\n",
    "    os.mkdir(\"./scalers\")\n",
    "\n",
    "\n",
    "num_cols = [\"_BMI5\", \"PHYSHLTH\", \"MENTHLTH\"]\n",
    "cat_cols = [x for x in df.columns if x not in [\"DIABETE3\"]+num_cols]\n",
    "\n",
    "df_train, df_test = df.randomSplit([.8, .2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9b6c766-69be-4460-8a1c-06180f4873aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pipeline from : ./scalers/_BMI5_pipeline\n",
      "Loading pipeline from : ./scalers/PHYSHLTH_pipeline\n",
      "Loading pipeline from : ./scalers/MENTHLTH_pipeline\n",
      "Loading pipeline from : ./scalers/_BMI5_pipeline\n",
      "Loading pipeline from : ./scalers/PHYSHLTH_pipeline\n",
      "Loading pipeline from : ./scalers/MENTHLTH_pipeline\n"
     ]
    }
   ],
   "source": [
    "df_train_scaled, train_new_feats = scale_data(df_train, num_cols)\n",
    "df_test_scaled, test_new_feats = scale_data(df_test, num_cols)\n",
    "\n",
    "df_train_scaled = df_train_scaled.withColumnRenamed(\"DIABETE3\", \"label\")\n",
    "df_test_scaled = df_test_scaled.withColumnRenamed(\"DIABETE3\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79346440-bd07-4fc6-afc1-50edb52120b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dtree = DecisionTreeClassifier(featuresCol=\"features\", labelCol=\"label\", seed=32)\n",
    "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\", seed=32)\n",
    "gbtree = GBTClassifier(featuresCol=\"features\", labelCol=\"label\", seed=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a8f8ba6-0dd7-4cd9-bb81-e149e4a38ccf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "paramGrid_dtree = ParamGridBuilder() \\\n",
    ".addGrid(dtree.maxDepth, [5, 10, 15, 20])\\\n",
    ".build()\n",
    "\n",
    "paramGrid_rf = ParamGridBuilder()\\\n",
    ".addGrid(rf.maxDepth, [5, 10, 15, 20])\\\n",
    ".addGrid(rf.numTrees, [100, 200, 300, 400])\\\n",
    ".build()\n",
    "\n",
    "paramGrid_gb = ParamGridBuilder()\\\n",
    ".addGrid(gbtree.maxDepth, [5, 10, 15, 20])\\\n",
    ".addGrid(gbtree.maxIter, [20, 50, 100, 300])\\\n",
    ".build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cba92253-d61b-4f3c-9e32-fd049c869d2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline_dtree = Pipeline(\n",
    "    stages=[\n",
    "        VectorAssembler(inputCols=cat_cols + train_new_feats, outputCol=\"features\"),\n",
    "        dtree\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipeline_rf = Pipeline(\n",
    "    stages=[\n",
    "        VectorAssembler(inputCols=cat_cols + train_new_feats, outputCol=\"features\"),\n",
    "        rf\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipeline_gbtree = Pipeline(\n",
    "    stages=[\n",
    "        VectorAssembler(inputCols=cat_cols + train_new_feats, outputCol=\"features\"),\n",
    "        gbtree\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f00de6-e175-48c9-90f1-426487c059bb",
   "metadata": {},
   "source": [
    "# Model Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f880c08f-8816-4cb1-a661-038c16427027",
   "metadata": {},
   "source": [
    "## - Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72c0c6c8-fed2-4878-9ffe-434c932962b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/12 20:40:58 WARN DAGScheduler: Broadcasting large task binary with size 1223.1 KiB\n",
      "23/10/12 20:40:58 WARN DAGScheduler: Broadcasting large task binary with size 1656.5 KiB\n",
      "23/10/12 20:40:58 WARN DAGScheduler: Broadcasting large task binary with size 1131.5 KiB\n",
      "23/10/12 20:41:00 WARN DAGScheduler: Broadcasting large task binary with size 1223.1 KiB\n",
      "23/10/12 20:41:00 WARN DAGScheduler: Broadcasting large task binary with size 1656.5 KiB\n",
      "23/10/12 20:41:00 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "23/10/12 20:41:00 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "23/10/12 20:41:00 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "23/10/12 20:41:01 WARN DAGScheduler: Broadcasting large task binary with size 3.9 MiB\n",
      "23/10/12 20:41:01 WARN DAGScheduler: Broadcasting large task binary with size 4.5 MiB\n",
      "23/10/12 20:41:01 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n",
      "23/10/12 20:41:06 WARN DAGScheduler: Broadcasting large task binary with size 1268.1 KiB\n",
      "23/10/12 20:41:07 WARN DAGScheduler: Broadcasting large task binary with size 1716.6 KiB\n",
      "23/10/12 20:41:07 WARN DAGScheduler: Broadcasting large task binary with size 1183.7 KiB\n",
      "23/10/12 20:41:08 WARN DAGScheduler: Broadcasting large task binary with size 1268.1 KiB\n",
      "23/10/12 20:41:08 WARN DAGScheduler: Broadcasting large task binary with size 1716.6 KiB\n",
      "23/10/12 20:41:08 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "23/10/12 20:41:08 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "23/10/12 20:41:09 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "23/10/12 20:41:09 WARN DAGScheduler: Broadcasting large task binary with size 3.9 MiB\n",
      "23/10/12 20:41:09 WARN DAGScheduler: Broadcasting large task binary with size 4.5 MiB\n",
      "23/10/12 20:41:10 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n",
      "23/10/12 20:41:15 WARN DAGScheduler: Broadcasting large task binary with size 1242.0 KiB\n",
      "23/10/12 20:41:15 WARN DAGScheduler: Broadcasting large task binary with size 1666.5 KiB\n",
      "23/10/12 20:41:15 WARN DAGScheduler: Broadcasting large task binary with size 1146.4 KiB\n",
      "23/10/12 20:41:16 WARN DAGScheduler: Broadcasting large task binary with size 1242.0 KiB\n",
      "23/10/12 20:41:16 WARN DAGScheduler: Broadcasting large task binary with size 1666.5 KiB\n",
      "23/10/12 20:41:16 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "23/10/12 20:41:16 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "23/10/12 20:41:17 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "23/10/12 20:41:17 WARN DAGScheduler: Broadcasting large task binary with size 3.8 MiB\n",
      "23/10/12 20:41:17 WARN DAGScheduler: Broadcasting large task binary with size 4.4 MiB\n",
      "23/10/12 20:41:18 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "crossval_dtree = CrossValidator(estimator=pipeline_dtree, \n",
    "                               estimatorParamMaps=paramGrid_dtree, \n",
    "                               evaluator=MulticlassClassificationEvaluator(),\n",
    "                               numFolds=3)\n",
    "cvmodel_dtree = crossval_dtree.fit(df_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f23e4040-22f5-4c0b-8599-7dc4d03eac6e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8025365038195532,\n",
       " 0.8070890667363112,\n",
       " 0.7998966711183496,\n",
       " 0.7839792735138795]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvmodel_dtree.avgMetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501de3bd-92f1-459e-b037-7ac483c2457c",
   "metadata": {},
   "source": [
    "## - Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85de93b5-8d55-4324-b359-bc490c81c851",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/12 20:42:01 WARN DAGScheduler: Broadcasting large task binary with size 1136.4 KiB\n",
      "23/10/12 20:42:08 WARN DAGScheduler: Broadcasting large task binary with size 1136.8 KiB\n",
      "23/10/12 20:42:15 WARN DAGScheduler: Broadcasting large task binary with size 1138.1 KiB\n",
      "23/10/12 20:42:24 WARN DAGScheduler: Broadcasting large task binary with size 1602.2 KiB\n",
      "23/10/12 20:42:27 WARN DAGScheduler: Broadcasting large task binary with size 1160.4 KiB\n",
      "23/10/12 20:42:35 WARN DAGScheduler: Broadcasting large task binary with size 1602.9 KiB\n",
      "23/10/12 20:42:37 WARN DAGScheduler: Broadcasting large task binary with size 1157.3 KiB\n",
      "23/10/12 20:42:45 WARN DAGScheduler: Broadcasting large task binary with size 1604.8 KiB\n",
      "23/10/12 20:42:48 WARN DAGScheduler: Broadcasting large task binary with size 1159.7 KiB\n",
      "23/10/12 20:42:55 WARN DAGScheduler: Broadcasting large task binary with size 1100.6 KiB\n",
      "23/10/12 20:42:58 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "23/10/12 20:43:02 WARN DAGScheduler: Broadcasting large task binary with size 1482.4 KiB\n",
      "23/10/12 20:43:09 WARN DAGScheduler: Broadcasting large task binary with size 1101.0 KiB\n",
      "23/10/12 20:43:12 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "23/10/12 20:43:15 WARN DAGScheduler: Broadcasting large task binary with size 1484.8 KiB\n",
      "23/10/12 20:43:22 WARN DAGScheduler: Broadcasting large task binary with size 1102.2 KiB\n",
      "23/10/12 20:43:26 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "23/10/12 20:43:29 WARN DAGScheduler: Broadcasting large task binary with size 1495.2 KiB\n",
      "23/10/12 20:43:33 WARN DAGScheduler: Broadcasting large task binary with size 1154.4 KiB\n",
      "23/10/12 20:43:34 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "23/10/12 20:43:36 WARN DAGScheduler: Broadcasting large task binary with size 3.9 MiB\n",
      "23/10/12 20:43:37 WARN DAGScheduler: Broadcasting large task binary with size 1159.8 KiB\n",
      "23/10/12 20:43:37 WARN DAGScheduler: Broadcasting large task binary with size 7.4 MiB\n",
      "23/10/12 20:43:39 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "23/10/12 20:43:40 WARN DAGScheduler: Broadcasting large task binary with size 13.4 MiB\n",
      "23/10/12 20:43:42 WARN DAGScheduler: Broadcasting large task binary with size 3.4 MiB\n",
      "23/10/12 20:43:43 WARN DAGScheduler: Broadcasting large task binary with size 7.5 MiB\n",
      "23/10/12 20:43:47 WARN DAGScheduler: Broadcasting large task binary with size 1155.2 KiB\n",
      "23/10/12 20:43:48 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "23/10/12 20:43:49 WARN DAGScheduler: Broadcasting large task binary with size 3.9 MiB\n",
      "23/10/12 20:43:51 WARN DAGScheduler: Broadcasting large task binary with size 1153.2 KiB\n",
      "23/10/12 20:43:51 WARN DAGScheduler: Broadcasting large task binary with size 7.4 MiB\n",
      "23/10/12 20:43:53 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "23/10/12 20:43:54 WARN DAGScheduler: Broadcasting large task binary with size 13.4 MiB\n",
      "23/10/12 20:43:56 WARN DAGScheduler: Broadcasting large task binary with size 3.4 MiB\n",
      "23/10/12 20:43:57 WARN DAGScheduler: Broadcasting large task binary with size 7.5 MiB\n",
      "23/10/12 20:44:01 WARN DAGScheduler: Broadcasting large task binary with size 1156.4 KiB\n",
      "23/10/12 20:44:02 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "23/10/12 20:44:03 WARN DAGScheduler: Broadcasting large task binary with size 3.9 MiB\n",
      "23/10/12 20:44:05 WARN DAGScheduler: Broadcasting large task binary with size 1156.9 KiB\n",
      "23/10/12 20:44:05 WARN DAGScheduler: Broadcasting large task binary with size 7.4 MiB\n",
      "23/10/12 20:44:07 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "23/10/12 20:44:08 WARN DAGScheduler: Broadcasting large task binary with size 13.4 MiB\n",
      "23/10/12 20:44:10 WARN DAGScheduler: Broadcasting large task binary with size 3.4 MiB\n",
      "23/10/12 20:44:11 WARN DAGScheduler: Broadcasting large task binary with size 7.5 MiB\n",
      "23/10/12 20:44:16 WARN DAGScheduler: Broadcasting large task binary with size 1136.4 KiB\n",
      "23/10/12 20:44:18 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "23/10/12 20:44:20 WARN DAGScheduler: Broadcasting large task binary with size 3.9 MiB\n",
      "23/10/12 20:44:23 WARN DAGScheduler: Broadcasting large task binary with size 1216.6 KiB\n",
      "23/10/12 20:44:23 WARN DAGScheduler: Broadcasting large task binary with size 7.7 MiB\n",
      "23/10/12 20:44:26 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "23/10/12 20:44:27 WARN DAGScheduler: Broadcasting large task binary with size 14.5 MiB\n",
      "23/10/12 20:44:31 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "23/10/12 20:44:33 WARN DAGScheduler: Broadcasting large task binary with size 26.6 MiB\n",
      "23/10/12 20:44:39 ERROR Executor: Exception in task 2.0 in stage 835.0 (TID 3596)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat scala.collection.immutable.HashMap$HashMap1.get0(HashMap.scala:359)\n",
      "\tat scala.collection.immutable.HashMap$HashTrieMap.get0(HashMap.scala:579)\n",
      "\tat scala.collection.immutable.HashMap$HashTrieMap.get0(HashMap.scala:574)\n",
      "\tat scala.collection.immutable.HashMap.get(HashMap.scala:70)\n",
      "\tat scala.collection.MapLike.apply(MapLike.scala:143)\n",
      "\tat scala.collection.MapLike.apply$(MapLike.scala:143)\n",
      "\tat scala.collection.AbstractMap.apply(Map.scala:65)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$8(RandomForest.scala:560)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$8$adapted(RandomForest.scala:558)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda$4947/0x0000000841a91840.apply(Unknown Source)\n",
      "\tat scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:400)\n",
      "\tat scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:728)\n",
      "\tat scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:728)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.binSeqOp$1(RandomForest.scala:558)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$24(RandomForest.scala:655)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda$4946/0x0000000841a91040.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$21(RandomForest.scala:655)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda$4932/0x0000000841a7a040.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$2947/0x00000008412a6840.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "23/10/12 20:44:39 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 2.0 in stage 835.0 (TID 3596),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat scala.collection.immutable.HashMap$HashMap1.get0(HashMap.scala:359)\n",
      "\tat scala.collection.immutable.HashMap$HashTrieMap.get0(HashMap.scala:579)\n",
      "\tat scala.collection.immutable.HashMap$HashTrieMap.get0(HashMap.scala:574)\n",
      "\tat scala.collection.immutable.HashMap.get(HashMap.scala:70)\n",
      "\tat scala.collection.MapLike.apply(MapLike.scala:143)\n",
      "\tat scala.collection.MapLike.apply$(MapLike.scala:143)\n",
      "\tat scala.collection.AbstractMap.apply(Map.scala:65)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$8(RandomForest.scala:560)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$8$adapted(RandomForest.scala:558)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda$4947/0x0000000841a91840.apply(Unknown Source)\n",
      "\tat scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:400)\n",
      "\tat scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:728)\n",
      "\tat scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:728)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.binSeqOp$1(RandomForest.scala:558)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$24(RandomForest.scala:655)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda$4946/0x0000000841a91040.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$21(RandomForest.scala:655)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda$4932/0x0000000841a7a040.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$2947/0x00000008412a6840.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "23/10/12 20:44:39 WARN TaskSetManager: Lost task 2.0 in stage 835.0 (TID 3596) (192.168.1.208 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat scala.collection.immutable.HashMap$HashMap1.get0(HashMap.scala:359)\n",
      "\tat scala.collection.immutable.HashMap$HashTrieMap.get0(HashMap.scala:579)\n",
      "\tat scala.collection.immutable.HashMap$HashTrieMap.get0(HashMap.scala:574)\n",
      "\tat scala.collection.immutable.HashMap.get(HashMap.scala:70)\n",
      "\tat scala.collection.MapLike.apply(MapLike.scala:143)\n",
      "\tat scala.collection.MapLike.apply$(MapLike.scala:143)\n",
      "\tat scala.collection.AbstractMap.apply(Map.scala:65)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$8(RandomForest.scala:560)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$8$adapted(RandomForest.scala:558)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda$4947/0x0000000841a91840.apply(Unknown Source)\n",
      "\tat scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:400)\n",
      "\tat scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:728)\n",
      "\tat scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:728)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.binSeqOp$1(RandomForest.scala:558)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$24(RandomForest.scala:655)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda$4946/0x0000000841a91040.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$21(RandomForest.scala:655)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda$4932/0x0000000841a7a040.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$2947/0x00000008412a6840.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\n",
      "23/10/12 20:44:39 ERROR TaskSetManager: Task 2 in stage 835.0 failed 1 times; aborting job\n",
      "23/10/12 20:44:39 WARN TaskSetManager: Lost task 3.0 in stage 835.0 (TID 3597) (192.168.1.208 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 2 in stage 835.0 failed 1 times, most recent failure: Lost task 2.0 in stage 835.0 (TID 3596) (192.168.1.208 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat scala.collection.immutable.HashMap$HashMap1.get0(HashMap.scala:359)\n",
      "\tat scala.collection.immutable.HashMap$HashTrieMap.get0(HashMap.scala:579)\n",
      "\tat scala.collection.immutable.HashMap$HashTrieMap.get0(HashMap.scala:574)\n",
      "\tat scala.collection.immutable.HashMap.get(HashMap.scala:70)\n",
      "\tat scala.collection.MapLike.apply(MapLike.scala:143)\n",
      "\tat scala.collection.MapLike.apply$(MapLike.scala:143)\n",
      "\tat scala.collection.AbstractMap.apply(Map.scala:65)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$8(RandomForest.scala:560)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$8$adapted(RandomForest.scala:558)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda$4947/0x0000000841a91840.apply(Unknown Source)\n",
      "\tat scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:400)\n",
      "\tat scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:728)\n",
      "\tat scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:728)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.binSeqOp$1(RandomForest.scala:558)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$24(RandomForest.scala:655)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda$4946/0x0000000841a91040.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$21(RandomForest.scala:655)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda$4932/0x0000000841a7a040.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$2947/0x00000008412a6840.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\n",
      "Driver stacktrace:)\n",
      "23/10/12 20:44:39 WARN TaskSetManager: Lost task 4.0 in stage 835.0 (TID 3598) (192.168.1.208 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 2 in stage 835.0 failed 1 times, most recent failure: Lost task 2.0 in stage 835.0 (TID 3596) (192.168.1.208 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat scala.collection.immutable.HashMap$HashMap1.get0(HashMap.scala:359)\n",
      "\tat scala.collection.immutable.HashMap$HashTrieMap.get0(HashMap.scala:579)\n",
      "\tat scala.collection.immutable.HashMap$HashTrieMap.get0(HashMap.scala:574)\n",
      "\tat scala.collection.immutable.HashMap.get(HashMap.scala:70)\n",
      "\tat scala.collection.MapLike.apply(MapLike.scala:143)\n",
      "\tat scala.collection.MapLike.apply$(MapLike.scala:143)\n",
      "\tat scala.collection.AbstractMap.apply(Map.scala:65)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$8(RandomForest.scala:560)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$8$adapted(RandomForest.scala:558)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda$4947/0x0000000841a91840.apply(Unknown Source)\n",
      "\tat scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:400)\n",
      "\tat scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:728)\n",
      "\tat scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:728)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.binSeqOp$1(RandomForest.scala:558)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$24(RandomForest.scala:655)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda$4946/0x0000000841a91040.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$21(RandomForest.scala:655)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda$4932/0x0000000841a7a040.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$2947/0x00000008412a6840.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\n",
      "Driver stacktrace:)\n",
      "23/10/12 20:44:39 ERROR Instrumentation: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 835.0 failed 1 times, most recent failure: Lost task 2.0 in stage 835.0 (TID 3596) (192.168.1.208 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat scala.collection.immutable.HashMap$HashMap1.get0(HashMap.scala:359)\n",
      "\tat scala.collection.immutable.HashMap$HashTrieMap.get0(HashMap.scala:579)\n",
      "\tat scala.collection.immutable.HashMap$HashTrieMap.get0(HashMap.scala:574)\n",
      "\tat scala.collection.immutable.HashMap.get(HashMap.scala:70)\n",
      "\tat scala.collection.MapLike.apply(MapLike.scala:143)\n",
      "\tat scala.collection.MapLike.apply$(MapLike.scala:143)\n",
      "\tat scala.collection.AbstractMap.apply(Map.scala:65)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$8(RandomForest.scala:560)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$8$adapted(RandomForest.scala:558)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda$4947/0x0000000841a91840.apply(Unknown Source)\n",
      "\tat scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:400)\n",
      "\tat scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:728)\n",
      "\tat scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:728)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.binSeqOp$1(RandomForest.scala:558)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$24(RandomForest.scala:655)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda$4946/0x0000000841a91040.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$21(RandomForest.scala:655)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda$4932/0x0000000841a7a040.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$2947/0x00000008412a6840.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1046)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1045)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$collectAsMap$1(PairRDDFunctions.scala:738)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:737)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.findBestSplits(RandomForest.scala:663)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.runBagged(RandomForest.scala:208)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:302)\n",
      "\tat org.apache.spark.ml.classification.RandomForestClassifier.$anonfun$train$1(RandomForestClassifier.scala:168)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:139)\n",
      "\tat org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:47)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:78)\n",
      "\tat jdk.internal.reflect.GeneratedMethodAccessor237.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.lang.OutOfMemoryError: Java heap space\n",
      "\tat scala.collection.immutable.HashMap$HashMap1.get0(HashMap.scala:359)\n",
      "\tat scala.collection.immutable.HashMap$HashTrieMap.get0(HashMap.scala:579)\n",
      "\tat scala.collection.immutable.HashMap$HashTrieMap.get0(HashMap.scala:574)\n",
      "\tat scala.collection.immutable.HashMap.get(HashMap.scala:70)\n",
      "\tat scala.collection.MapLike.apply(MapLike.scala:143)\n",
      "\tat scala.collection.MapLike.apply$(MapLike.scala:143)\n",
      "\tat scala.collection.AbstractMap.apply(Map.scala:65)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$8(RandomForest.scala:560)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$8$adapted(RandomForest.scala:558)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda$4947/0x0000000841a91840.apply(Unknown Source)\n",
      "\tat scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:400)\n",
      "\tat scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:728)\n",
      "\tat scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:728)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.binSeqOp$1(RandomForest.scala:558)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$24(RandomForest.scala:655)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda$4946/0x0000000841a91040.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$21(RandomForest.scala:655)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda$4932/0x0000000841a7a040.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$2947/0x00000008412a6840.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\n",
      "23/10/12 20:44:39 WARN TaskSetManager: Lost task 0.0 in stage 835.0 (TID 3594) (192.168.1.208 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 2 in stage 835.0 failed 1 times, most recent failure: Lost task 2.0 in stage 835.0 (TID 3596) (192.168.1.208 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat scala.collection.immutable.HashMap$HashMap1.get0(HashMap.scala:359)\n",
      "\tat scala.collection.immutable.HashMap$HashTrieMap.get0(HashMap.scala:579)\n",
      "\tat scala.collection.immutable.HashMap$HashTrieMap.get0(HashMap.scala:574)\n",
      "\tat scala.collection.immutable.HashMap.get(HashMap.scala:70)\n",
      "\tat scala.collection.MapLike.apply(MapLike.scala:143)\n",
      "\tat scala.collection.MapLike.apply$(MapLike.scala:143)\n",
      "\tat scala.collection.AbstractMap.apply(Map.scala:65)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$8(RandomForest.scala:560)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$8$adapted(RandomForest.scala:558)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda$4947/0x0000000841a91840.apply(Unknown Source)\n",
      "\tat scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:400)\n",
      "\tat scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:728)\n",
      "\tat scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:728)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.binSeqOp$1(RandomForest.scala:558)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$24(RandomForest.scala:655)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda$4946/0x0000000841a91040.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$21(RandomForest.scala:655)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda$4932/0x0000000841a7a040.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$2947/0x00000008412a6840.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\n",
      "Driver stacktrace:)\n",
      "23/10/12 20:44:39 WARN TaskSetManager: Lost task 1.0 in stage 835.0 (TID 3595) (192.168.1.208 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 2 in stage 835.0 failed 1 times, most recent failure: Lost task 2.0 in stage 835.0 (TID 3596) (192.168.1.208 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat scala.collection.immutable.HashMap$HashMap1.get0(HashMap.scala:359)\n",
      "\tat scala.collection.immutable.HashMap$HashTrieMap.get0(HashMap.scala:579)\n",
      "\tat scala.collection.immutable.HashMap$HashTrieMap.get0(HashMap.scala:574)\n",
      "\tat scala.collection.immutable.HashMap.get(HashMap.scala:70)\n",
      "\tat scala.collection.MapLike.apply(MapLike.scala:143)\n",
      "\tat scala.collection.MapLike.apply$(MapLike.scala:143)\n",
      "\tat scala.collection.AbstractMap.apply(Map.scala:65)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$8(RandomForest.scala:560)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$8$adapted(RandomForest.scala:558)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda$4947/0x0000000841a91840.apply(Unknown Source)\n",
      "\tat scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:400)\n",
      "\tat scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:728)\n",
      "\tat scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:728)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.binSeqOp$1(RandomForest.scala:558)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$24(RandomForest.scala:655)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda$4946/0x0000000841a91040.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$21(RandomForest.scala:655)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda$4932/0x0000000841a7a040.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$2947/0x00000008412a6840.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\n",
      "Driver stacktrace:)\n",
      "23/10/12 20:44:39 ERROR Instrumentation: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\n",
      "This stopped SparkContext was created at:\n",
      "\n",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "py4j.Gateway.invoke(Gateway.java:238)\n",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "The currently active SparkContext was created at:\n",
      "\n",
      "(No active SparkContext.)\n",
      "         \n",
      "\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:122)\n",
      "\tat org.apache.spark.SparkContext.defaultParallelism(SparkContext.scala:2707)\n",
      "\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.$anonfun$apply$1(CoalesceShufflePartitions.scala:60)\n",
      "\tat scala.runtime.java8.JFunction0$mcI$sp.apply(JFunction0$mcI$sp.java:23)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.apply(CoalesceShufflePartitions.scala:57)\n",
      "\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.apply(CoalesceShufflePartitions.scala:33)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$optimizeQueryStage$1(AdaptiveSparkPlanExec.scala:163)\n",
      "\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n",
      "\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n",
      "\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.optimizeQueryStage(AdaptiveSparkPlanExec.scala:162)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.newQueryStage(AdaptiveSparkPlanExec.scala:595)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:547)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:561)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:561)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:561)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:561)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:561)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:561)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:261)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:256)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:401)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.doExecute(AdaptiveSparkPlanExec.scala:386)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:207)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:206)\n",
      "\tat org.apache.spark.sql.Dataset.rdd$lzycompute(Dataset.scala:3861)\n",
      "\tat org.apache.spark.sql.Dataset.rdd(Dataset.scala:3859)\n",
      "\tat org.apache.spark.ml.util.Instrumentation.logDataset(Instrumentation.scala:62)\n",
      "\tat org.apache.spark.ml.classification.RandomForestClassifier.$anonfun$train$1(RandomForestClassifier.scala:141)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:139)\n",
      "\tat org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:47)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:78)\n",
      "\tat jdk.internal.reflect.GeneratedMethodAccessor237.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "23/10/12 20:44:39 ERROR Instrumentation: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\n",
      "This stopped SparkContext was created at:\n",
      "\n",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "py4j.Gateway.invoke(Gateway.java:238)\n",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "The currently active SparkContext was created at:\n",
      "\n",
      "(No active SparkContext.)\n",
      "         \n",
      "\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:122)\n",
      "\tat org.apache.spark.SparkContext.defaultParallelism(SparkContext.scala:2707)\n",
      "\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.$anonfun$apply$1(CoalesceShufflePartitions.scala:60)\n",
      "\tat scala.runtime.java8.JFunction0$mcI$sp.apply(JFunction0$mcI$sp.java:23)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.apply(CoalesceShufflePartitions.scala:57)\n",
      "\tat org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions.apply(CoalesceShufflePartitions.scala:33)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$optimizeQueryStage$1(AdaptiveSparkPlanExec.scala:163)\n",
      "\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n",
      "\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n",
      "\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.optimizeQueryStage(AdaptiveSparkPlanExec.scala:162)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.newQueryStage(AdaptiveSparkPlanExec.scala:595)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:547)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:561)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:561)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:561)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:561)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:561)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:561)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:261)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:256)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:401)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.doExecute(AdaptiveSparkPlanExec.scala:386)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:207)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:206)\n",
      "\tat org.apache.spark.sql.Dataset.rdd$lzycompute(Dataset.scala:3861)\n",
      "\tat org.apache.spark.sql.Dataset.rdd(Dataset.scala:3859)\n",
      "\tat org.apache.spark.ml.util.Instrumentation.logDataset(Instrumentation.scala:62)\n",
      "\tat org.apache.spark.ml.classification.RandomForestClassifier.$anonfun$train$1(RandomForestClassifier.scala:141)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:139)\n",
      "\tat org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:47)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:78)\n",
      "\tat jdk.internal.reflect.GeneratedMethodAccessor237.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 41758)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/darthvader/miniconda3/envs/expenv/lib/python3.11/socketserver.py\", line 317, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/home/darthvader/miniconda3/envs/expenv/lib/python3.11/socketserver.py\", line 348, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/home/darthvader/miniconda3/envs/expenv/lib/python3.11/socketserver.py\", line 361, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/home/darthvader/miniconda3/envs/expenv/lib/python3.11/socketserver.py\", line 755, in __init__\n",
      "    self.handle()\n",
      "  File \"/opt/spark/python/pyspark/accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/opt/spark/python/pyspark/accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "                           ^^^^^^\n",
      "  File \"/opt/spark/python/pyspark/accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/spark/python/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: <exception str() failed>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/darthvader/miniconda3/envs/expenv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_57505/3652014620.py\", line 5, in <module>\n",
      "    cvmodel_rf = crossval_rf.fit(df_train_scaled)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/spark/python/pyspark/ml/base.py\", line 205, in fit\n",
      "    return self._fit(dataset)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/spark/python/pyspark/ml/tuning.py\", line 847, in _fit\n",
      "    for j, metric, subModel in pool.imap_unordered(lambda f: f(), tasks):\n",
      "  File \"/home/darthvader/miniconda3/envs/expenv/lib/python3.11/multiprocessing/pool.py\", line 873, in next\n",
      "    raise value\n",
      "  File \"/home/darthvader/miniconda3/envs/expenv/lib/python3.11/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "                    ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/spark/python/pyspark/ml/tuning.py\", line 847, in <lambda>\n",
      "    for j, metric, subModel in pool.imap_unordered(lambda f: f(), tasks):\n",
      "                                                             ^^^\n",
      "  File \"/opt/spark/python/pyspark/util.py\", line 342, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/spark/python/pyspark/ml/tuning.py\", line 113, in singleTask\n",
      "    index, model = next(modelIter)\n",
      "                   ^^^^^^^^^^^^^^^\n",
      "  File \"/opt/spark/python/pyspark/ml/base.py\", line 98, in __next__\n",
      "    return index, self.fitSingleModel(index)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/spark/python/pyspark/ml/base.py\", line 156, in fitSingleModel\n",
      "    return estimator.fit(dataset, paramMaps[index])\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/spark/python/pyspark/ml/base.py\", line 203, in fit\n",
      "    return self.copy(params)._fit(dataset)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/spark/python/pyspark/ml/pipeline.py\", line 134, in _fit\n",
      "    model = stage.fit(dataset)\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/spark/python/pyspark/ml/base.py\", line 205, in fit\n",
      "    return self._fit(dataset)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/spark/python/pyspark/ml/wrapper.py\", line 381, in _fit\n",
      "    java_model = self._fit_java(dataset)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/spark/python/pyspark/ml/wrapper.py\", line 378, in _fit_java\n",
      "    return self._java_obj.fit(dataset._jdf)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "                   ^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/spark/python/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: <exception str() failed>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 111] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[15], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m crossval_rf \u001b[38;5;241m=\u001b[39m CrossValidator(estimator\u001b[38;5;241m=\u001b[39mpipeline_rf, \n\u001b[1;32m      2\u001b[0m                                estimatorParamMaps\u001b[38;5;241m=\u001b[39mparamGrid_rf, \n\u001b[1;32m      3\u001b[0m                                evaluator\u001b[38;5;241m=\u001b[39mMulticlassClassificationEvaluator(),\n\u001b[1;32m      4\u001b[0m                                numFolds\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m cvmodel_rf \u001b[38;5;241m=\u001b[39m \u001b[43mcrossval_rf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_train_scaled\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/ml/tuning.py:847\u001b[0m, in \u001b[0;36mCrossValidator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    843\u001b[0m tasks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(\n\u001b[1;32m    844\u001b[0m     inheritable_thread_target,\n\u001b[1;32m    845\u001b[0m     _parallelFitTasks(est, train, eva, validation, epm, collectSubModelsParam),\n\u001b[1;32m    846\u001b[0m )\n\u001b[0;32m--> 847\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j, metric, subModel \u001b[38;5;129;01min\u001b[39;00m pool\u001b[38;5;241m.\u001b[39mimap_unordered(\u001b[38;5;28;01mlambda\u001b[39;00m f: f(), tasks):\n\u001b[1;32m    848\u001b[0m     metrics_all[i][j] \u001b[38;5;241m=\u001b[39m metric\n",
      "File \u001b[0;32m~/miniconda3/envs/expenv/lib/python3.11/multiprocessing/pool.py:873\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    872\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m value\n\u001b[0;32m--> 873\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m value\n",
      "File \u001b[0;32m~/miniconda3/envs/expenv/lib/python3.11/multiprocessing/pool.py:125\u001b[0m, in \u001b[0;36mworker\u001b[0;34m(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 125\u001b[0m     result \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/ml/tuning.py:847\u001b[0m, in \u001b[0;36mCrossValidator._fit.<locals>.<lambda>\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    843\u001b[0m tasks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(\n\u001b[1;32m    844\u001b[0m     inheritable_thread_target,\n\u001b[1;32m    845\u001b[0m     _parallelFitTasks(est, train, eva, validation, epm, collectSubModelsParam),\n\u001b[1;32m    846\u001b[0m )\n\u001b[0;32m--> 847\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j, metric, subModel \u001b[38;5;129;01min\u001b[39;00m pool\u001b[38;5;241m.\u001b[39mimap_unordered(\u001b[38;5;28;01mlambda\u001b[39;00m f: \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, tasks):\n\u001b[1;32m    848\u001b[0m     metrics_all[i][j] \u001b[38;5;241m=\u001b[39m metric\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/util.py:342\u001b[0m, in \u001b[0;36minheritable_thread_target.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    341\u001b[0m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\u001b[38;5;241m.\u001b[39m_jsc\u001b[38;5;241m.\u001b[39msc()\u001b[38;5;241m.\u001b[39msetLocalProperties(properties)\n\u001b[0;32m--> 342\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/ml/tuning.py:113\u001b[0m, in \u001b[0;36m_parallelFitTasks.<locals>.singleTask\u001b[0;34m()\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msingleTask\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m, Transformer]:\n\u001b[0;32m--> 113\u001b[0m     index, model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(modelIter)\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;66;03m# TODO: duplicate evaluator to take extra params from input\u001b[39;00m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m#  Note: Supporting tuning params in evaluator need update method\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m#  `MetaAlgorithmReadWrite.getAllNestedStages`, make it return\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m#  all nested stages and evaluators\u001b[39;00m\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/ml/base.py:98\u001b[0m, in \u001b[0;36m_FitMultipleIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcounter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m index, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfitSingleModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/ml/base.py:156\u001b[0m, in \u001b[0;36mEstimator.fitMultiple.<locals>.fitSingleModel\u001b[0;34m(index)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfitSingleModel\u001b[39m(index: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m M:\n\u001b[0;32m--> 156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparamMaps\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/ml/base.py:203\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m params:\n\u001b[0;32m--> 203\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/ml/pipeline.py:134\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# must be an Estimator\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mstage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m     transformers\u001b[38;5;241m.\u001b[39mappend(model)\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/ml/wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[0;32m--> 381\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    382\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/ml/wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31m<class 'str'>\u001b[0m: (<class 'ConnectionRefusedError'>, ConnectionRefusedError(111, 'Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/expenv/lib/python3.11/site-packages/IPython/core/interactiveshell.py:2116\u001b[0m, in \u001b[0;36mInteractiveShell.showtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2113\u001b[0m     traceback\u001b[38;5;241m.\u001b[39mprint_exc()\n\u001b[1;32m   2114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2116\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_showtraceback\u001b[49m\u001b[43m(\u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_pdb:\n\u001b[1;32m   2118\u001b[0m     \u001b[38;5;66;03m# drop into debugger\u001b[39;00m\n\u001b[1;32m   2119\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebugger(force\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/expenv/lib/python3.11/site-packages/ipykernel/zmqshell.py:556\u001b[0m, in \u001b[0;36mZMQInteractiveShell._showtraceback\u001b[0;34m(self, etype, evalue, stb)\u001b[0m\n\u001b[1;32m    550\u001b[0m sys\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m    551\u001b[0m sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m    553\u001b[0m exc_content \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraceback\u001b[39m\u001b[38;5;124m\"\u001b[39m: stb,\n\u001b[1;32m    555\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mename\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(etype\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m),\n\u001b[0;32m--> 556\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevalue\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    557\u001b[0m }\n\u001b[1;32m    559\u001b[0m dh \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplayhook\n\u001b[1;32m    560\u001b[0m \u001b[38;5;66;03m# Send exception info over pub socket for other clients than the caller\u001b[39;00m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;66;03m# to pick up\u001b[39;00m\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:471\u001b[0m, in \u001b[0;36mPy4JJavaError.__str__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__str__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    470\u001b[0m     gateway_client \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_exception\u001b[38;5;241m.\u001b[39m_gateway_client\n\u001b[0;32m--> 471\u001b[0m     answer \u001b[38;5;241m=\u001b[39m \u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexception_cmd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    472\u001b[0m     return_value \u001b[38;5;241m=\u001b[39m get_return_value(answer, gateway_client, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    473\u001b[0m     \u001b[38;5;66;03m# Note: technically this should return a bytestring 'str' rather than\u001b[39;00m\n\u001b[1;32m    474\u001b[0m     \u001b[38;5;66;03m# unicodes in Python 2; however, it can return unicodes for now.\u001b[39;00m\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;66;03m# See https://github.com/bartdag/py4j/issues/306 for more details.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    288\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 291\u001b[0m     \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[0;32m--> 438\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused"
     ]
    }
   ],
   "source": [
    "crossval_rf = CrossValidator(estimator=pipeline_rf, \n",
    "                               estimatorParamMaps=paramGrid_rf, \n",
    "                               evaluator=MulticlassClassificationEvaluator(),\n",
    "                               numFolds=3)\n",
    "cvmodel_rf = crossval_rf.fit(df_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fe9446-fa66-4cec-9d70-ed2ca193cd4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cvmodel_rf.avgMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c235a4-1b48-4fc0-a3e3-26321ea00f58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "crossval_gbtree = CrossValidator(estimator=pipeline_gbtree, \n",
    "                               estimatorParamMaps=paramGrid_gbtree, \n",
    "                               evaluator=MulticlassClassificationEvaluator(),\n",
    "                               numFolds=3)\n",
    "cvmodel_gbtree = crossval_gbtree.fit(df_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b720820-23d8-4911-abcc-072088660386",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 37200)\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/darthvader/miniconda3/envs/expenv/lib/python3.11/socketserver.py\", line 317, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/home/darthvader/miniconda3/envs/expenv/lib/python3.11/socketserver.py\", line 348, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/home/darthvader/miniconda3/envs/expenv/lib/python3.11/socketserver.py\", line 361, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/home/darthvader/miniconda3/envs/expenv/lib/python3.11/socketserver.py\", line 755, in __init__\n",
      "    self.handle()\n",
      "  File \"/opt/spark/python/pyspark/accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/opt/spark/python/pyspark/accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "                           ^^^^^^\n",
      "  File \"/opt/spark/python/pyspark/accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/spark/python/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    }
   ],
   "source": [
    "cvmodel_gbtree.avgMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa8e947-5ab4-4530-8444-8426aac9b165",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
